# Optimization Techniques {#sec:opti}

## Momentum

### Nesterov's Accelerated Gradients

## RMSProp


## Adam


## AMSGrad



<!--

## Initialization
## Normalization







## Input Normalization

I provided *reasonable* ranges for values in the previous code example. For example, temperature values on Earth are typically in the range $[-20, 40]$ Â°C and illuminance in the range $[0, 1e6]$ Lux.


An NN can work with with values in these ranges, but it makes learning easier when you first scale values into the same range, typically $[-1, 1]$. TODO: why?


## Parameter Initialization

TODO: why can we start b at 0 by not \mathbf{w}?

## Vanishing and Exploding Gradients

https://nbviewer.jupyter.org/gist/joshfp/85d96f07aaa5f4d2c9eb47956ccdcc88/lesson2-sgd-in-action.ipynb



-->
