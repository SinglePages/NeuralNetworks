<h1 id="sec:intro">Introduction</h1>
<p>Goal: provide a concise walk-through of all fundamental neural network (including modern deep learning) techniques.</p>
<p>I will not discuss every possible analogy, angle, or topic here. Instead, I will provide links to external resources so that you can choose which topics you want to investigate more closely. I will provide minimal code examples when appropriate.</p>
<p><strong>Useful prior knowledge:</strong></p>
<ul>
<li>matrix calculus
<ul>
<li>see <a href="https://explained.ai/matrix-calculus/">The Matrix Calculus You Need For Deep Learning</a> by Terence Parr and Jeremy Howard</li>
</ul></li>
<li>programming skills
<ul>
<li>I will show examples in Python, but many languages will work</li>
</ul></li>
<li>familiarity with computing tools
<ul>
<li>using a server, cloud-based services, the command line interface (CLI)</li>
</ul></li>
</ul>
<h2 id="background">Background</h2>
<p>TODO: background</p>
<ul>
<li>AI/ML/NN</li>
<li>automatic features</li>
<li>supervised/unsupervised/rl</li>
<li>applications</li>
<li>terminology (nn, ann, mlp)</li>
<li>ethics</li>
<li>non-ml example</li>
</ul>
<p>Application areas</p>
<ul>
<li>Collaborative Filtering</li>
</ul>
<h1 id="sec:ethics">Ethics</h1>
<h1 id="sec:data">Data</h1>
<p>Often considered the most important aspect of deep learning,</p>
<p><span class="math display">ùíü‚ÄÑ=‚ÄÑ{<em>X</em>,‚ÄÜ<em>Y</em>}</span></p>
<!-- TODO: address training/validation/test sets -->
<p>is a dataset comprising input <em>features</em> <span class="math inline"><em>X</em></span> and output <em>targets</em> <span class="math inline"><em>Y</em></span>. Although <span class="math inline"><em>X</em></span> and <span class="math inline"><em>Y</em></span> can come in many shapes, I am going to be opinionated here and use a specific (and consistent) convention. Let‚Äôs use <span class="math inline"><em>N</em></span> to denote the size of the paired dataset. (Note, not all problems have output targets, but herein I am talking about supervised learning unless otherwise specified.)</p>
<p><span class="math inline"><em>X</em></span> is a matrix (indicated by capitalization) containing all features of all input examples. A single input example <span class="math inline"><strong>x</strong><sup>(<em>i</em>)</sup></span> is often represented as a <em>column</em> vector (indicated by boldface).</p>
<p><span class="math display">$$
\mathbf{x}^{(i)} =
\begin{bmatrix}
x^{(i)}_{1} \\
x^{(i)}_{2} \\
\vdots \\
x^{(i)}_{n_x-1} \\
x^{(i)}_{n_x} \\
\end{bmatrix}
$$</span></p>
<p>where <span class="math inline"><em>n</em><sub><em>x</em></sub></span> is the number of input features. We do not always put the input features into a column vector (see <span class="citation" data-cites="sec:cnns">[@sec:cnns]</span> for more information), but it is a useful convention to remember.</p>
<p>Each row in <span class="math inline"><em>X</em></span> is a single input example (also referred to as an instance or sample), and when you stack all <span class="math inline"><em>N</em></span> examples side-by-side, you end up with</p>
<p><span class="math display">$$
X =
\begin{bmatrix}
\mathbf{x}^{(1)T}\\
\mathbf{x}^{(2)T}\\
\vdots\\
\mathbf{x}^{(N)T}
\end{bmatrix}
=
\begin{bmatrix}
x^{(1)}_{1} &amp; x^{(1)}_{2} &amp; \cdots &amp; x^{(1)}_{n_x-1} &amp; x^{(1)}_{n_x}\\
x^{(2)}_{1} &amp; x^{(2)}_{2} &amp; \cdots &amp; x^{(2)}_{n_x-1} &amp; x^{(2)}_{n_x}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
x^{(N-1)}_{1} &amp; x^{(N-1)}_{2} &amp; \cdots &amp; x^{(N-1)}_{n_x-1} &amp; x^{(N-1)}_{n_x}\\
x^{(N)}_{1} &amp; x^{(N)}_{2} &amp; \cdots &amp; x^{(N)}_{n_x-1} &amp; x^{(N)}_{n_x}\\
\end{bmatrix}.
$$</span></p>
<!-- TODO: insert equations using m4 -->
<p>We need to transpose each example column vector (i.e., <span class="math inline"><strong>x</strong><sup>(1)<em>T</em></sup></span>) into a row vector so that the first dimension of <span class="math inline"><em>X</em></span> is the number of examples <span class="math inline"><em>N</em></span> and the second dimension is the number of features <span class="math inline"><em>n</em><sub><em>n</em><sub><em>x</em></sub></sub></span>. (This is not required, but it is the convention I will use for <span class="math inline"><em>X</em></span>.)</p>
<p>We say that <span class="math inline"><strong>x</strong><sup>(<em>i</em>)</sup>‚ÄÑ‚àà‚ÄÑ‚Ñõ<sup><em>n</em><sub><em>x</em></sub></sup></span> (each input example is <span class="math inline"><em>n</em><sub><em>x</em></sub></span> real values) and <span class="math inline"><em>X</em>‚ÄÑ‚àà‚ÄÑ‚Ñõ<sup><em>N</em>‚ÄÖ√ó‚ÄÖ<em>n</em><sub><em>x</em></sub></sup></span> (the entire input is a <span class="math inline">(<em>N</em>,<em>n</em><sub><em>x</em></sub>)</span> matrix).</p>
<p><span class="math inline"><em>Y</em></span> contains the targets (also referred to as labels or the true/correct/expected output values).</p>
<p><span class="math display">$$
Y =
\begin{bmatrix}
\mathbf{y}^{(1)T}\\
\mathbf{y}^{(2)T}\\
\vdots\\
\mathbf{y}^{(N)T}
\end{bmatrix}
=
\begin{bmatrix}
y^{(1)}_{1} &amp; y^{(1)}_{2} &amp; \cdots &amp; y^{(1)}_{n_y-1} &amp; y^{(1)}_{n_y}\\
y^{(2)}_{1} &amp; y^{(2)}_{2} &amp; \cdots &amp; y^{(2)}_{n_y-1} &amp; y^{(2)}_{n_y}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
y^{(N-1)}_{1} &amp; y^{(N-1)}_{2} &amp; \cdots &amp; y^{(N-1)}_{n_y-1} &amp; y^{(N-1)}_{n_y}\\
y^{(N)}_{1} &amp; y^{(N)}_{2} &amp; \cdots &amp; y^{(N)}_{n_y-1} &amp; y^{(N)}_{n_y}\\
\end{bmatrix}
$$</span></p>
<p>Each <span class="math inline"><em>y</em><sup>(<em>i</em>)</sup>‚ÄÑ‚àà‚ÄÑ‚Ñõ<sup><em>n</em><sub><em>y</em></sub></sup></span> (each target is <span class="math inline"><em>n</em><sub><em>y</em></sub></span> real values) and <span class="math inline"><em>Y</em>‚ÄÑ‚àà‚ÄÑ‚Ñõ<sup><em>N</em>‚ÄÖ√ó‚ÄÖ<em>n</em><sub><em>y</em></sub></sup></span> (the entire input is a <span class="math inline">(<em>N</em>,<em>n</em><sub><em>y</em></sub>)</span> matrix).</p>
<p>For example, we might <strong>predict a person‚Äôs location on Earth in latitude, longitude, and altitude by looking at the temperature, illuminance, time of day, and day of year at their location</strong>. In this example, <span class="math inline"><em>n</em><sub><em>x</em></sub></span> and <span class="math inline"><em>n</em><sub><em>y</em></sub></span> are <span class="math inline">4</span> (temperature, illuminance, time of day, and day of year) and <span class="math inline">3</span> (latitude, longitude, and altitude), respectively. And if we have <span class="math inline"><em>N</em>‚ÄÑ=‚ÄÑ785</span> example pairs, then <span class="math inline"><em>X</em></span> and <span class="math inline"><em>Y</em></span> are <span class="math inline">(785,4)</span> and <span class="math inline">(785,3)</span>, respectively.</p>
<h1 id="single-neuron">Single Neuron</h1>
<p>When our model is a single neuron we can only produce a single output. So, <span class="math inline"><em>n</em><sub><em>y</em></sub>‚ÄÑ=‚ÄÑ1</span> for this section.</p>
<h2 id="notation-and-diagram">Notation and Diagram</h2>
<p>A diagram representing a single neuron (as we‚Äôll see later, a neural network often refers to many of these neurons interconnected):</p>
<!-- TODO: use m4 here -->
<!-- Generated by graphviz version 2.47.0 (20210316.0004)
 -->
<!-- Title: SingleNeuron Pages: 1 -->
<svg width="431pt" height="253pt" viewBox="0.00 0.00 431.00 253.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 249)">
<title>
SingleNeuron
</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-249 427,-249 427,4 -4,4"/> <g id="clust1" class="cluster">
<title>
cluster_input
</title>
<polygon fill="none" stroke="white" points="0,-8 0,-237 95,-237 95,-8 0,-8"/> <text text-anchor="middle" x="47.5" y="-221.8" font-family="Times,serif" font-size="14.00">Input Features</text> </g> <g id="clust2" class="cluster">
<title>
cluster_linear
</title>
<polygon fill="none" stroke="white" points="177,-47 177,-187 229,-187 229,-47 177,-47"/> <text text-anchor="middle" x="203" y="-171.8" font-family="Times,serif" font-size="14.00">Linear</text> </g> <g id="clust3" class="cluster">
<title>
cluster_activation
</title>
<polygon fill="none" stroke="white" points="250,-112 250,-187 324,-187 324,-112 250,-112"/> <text text-anchor="middle" x="287" y="-171.8" font-family="Times,serif" font-size="14.00">Activation</text> </g> <g id="clust4" class="cluster">
<title>
cluster_output
</title>
<polygon fill="none" stroke="white" points="345,-112 345,-164 415,-164 415,-112 345,-112"/> </g> <!-- x1 --> <g id="node1" class="node">
<title>
x1
</title>
<ellipse fill="none" stroke="lightgray" cx="47" cy="-188" rx="18" ry="18"/> <text text-anchor="middle" x="47" y="-184.3" font-family="Times,serif" font-size="14.00">&amp;x_1</text> </g> <!-- z --> <g id="node7" class="node">
<title>
z
</title>
<ellipse fill="none" stroke="#ee0000" cx="203" cy="-138" rx="18" ry="18"/> <text text-anchor="middle" x="203" y="-134.3" font-family="Times,serif" font-size="14.00">&amp;z</text> </g> <!-- x1&#45;&#45;z --> <g id="edge1" class="edge">
<title>
x1--z
</title>
<path fill="none" stroke="black" d="M64.48,-182.65C94.12,-173.03 156.08,-152.91 185.64,-143.31"/> <text text-anchor="middle" x="136" y="-175.8" font-family="Times,serif" font-size="14.00">&amp;w_1</text> </g> <!-- x2 --> <g id="node2" class="node">
<title>
x2
</title>
<ellipse fill="none" stroke="lightgray" cx="47" cy="-138" rx="18" ry="18"/> <text text-anchor="middle" x="47" y="-134.3" font-family="Times,serif" font-size="14.00">&amp;x_2</text> </g> <!-- x2&#45;&#45;z --> <g id="edge2" class="edge">
<title>
x2--z
</title>
<path fill="none" stroke="black" d="M65.07,-138C94.67,-138 155.2,-138 184.85,-138"/> <text text-anchor="middle" x="136" y="-141.8" font-family="Times,serif" font-size="14.00">&amp;w_2</text> </g> <!-- e1 --> <g id="node3" class="node">
<title>
e1
</title>
<ellipse fill="black" stroke="black" cx="47" cy="-104" rx="1.8" ry="1.8"/> </g> <!-- e2 --> <g id="node4" class="node">
<title>
e2
</title>
<ellipse fill="black" stroke="black" cx="47" cy="-86" rx="1.8" ry="1.8"/> </g> <!-- e3 --> <g id="node5" class="node">
<title>
e3
</title>
<ellipse fill="black" stroke="black" cx="47" cy="-68" rx="1.8" ry="1.8"/> </g> <!-- xn --> <g id="node6" class="node needsmath">
<title>
xn
</title>
<ellipse fill="none" stroke="lightgray" cx="47" cy="-34" rx="18" ry="18"/> <text text-anchor="middle" x="47" y="-30.3" font-family="Times,serif" font-size="14.00">&amp;x_{n_x}</text> </g> <!-- xn&#45;&#45;z --> <g id="edge3" class="edge">
<title>
xn--z
</title>
<path fill="none" stroke="black" d="M62.49,-43.78C91.84,-63.61 158.03,-108.3 187.44,-128.17"/> <text text-anchor="middle" x="136" y="-113.8" font-family="Times,serif" font-size="14.00">&amp;w_{n_x}</text> </g> <!-- b --> <g id="node8" class="node">
<title>
b
</title>
<ellipse fill="none" stroke="lightgray" cx="203" cy="-73" rx="18" ry="18"/> <text text-anchor="middle" x="203" y="-69.3" font-family="Times,serif" font-size="14.00">&amp;1</text> </g> <!-- z&#45;&#45;b --> <g id="edge5" class="edge">
<title>
z--b
</title>
<path fill="none" stroke="black" d="M203,-119.99C203,-110.99 203,-100.12 203,-91.11"/> <text text-anchor="middle" x="192.5" y="-101.8" font-family="Times,serif" font-size="14.00">&amp;b</text> </g> <!-- a --> <g id="node9" class="node">
<title>
a
</title>
<ellipse fill="none" stroke="green" cx="287" cy="-138" rx="18" ry="18"/> <text text-anchor="middle" x="287" y="-134.3" font-family="Times,serif" font-size="14.00">&amp;a</text> </g> <!-- z&#45;&#45;a --> <g id="edge4" class="edge">
<title>
z--a
</title>
<path fill="none" stroke="black" d="M221.39,-138C235.4,-138 254.99,-138 268.92,-138"/> </g> <!-- yhat --> <g id="node10" class="node">
<title>
yhat
</title>
<text text-anchor="middle" x="380" y="-134.3" font-family="Times,serif" font-size="14.00">&amp;≈∑</text> </g> <!-- a&#45;&#45;yhat --> <g id="edge6" class="edge">
<title>
a--yhat
</title>
<path fill="none" stroke="black" d="M305.12,-138C318.5,-138 337.35,-138 352.65,-138"/> </g> </g>
</svg>
<script type="text/javascript" src="js/main.js"></script>
<p><link rel="stylesheet" href="css/main.css"></p>
<p>The diagram represents the following equations:</p>
<p></p>
<p>The main points of this equation:</p>
<ul>
<li><span class="math inline"><em>x</em><sub><em>k</em></sub><sup>(<em>i</em>)</sup></span> are the input features for the <span class="math inline"><em>i</em><sup><em>t</em><em>h</em></sup></span> example (e.g., temperature)</li>
<li><span class="math inline"><em>z</em><sup>(<em>i</em>)</sup></span> is a linear combination of the input features</li>
<li><span class="math inline"><em>w</em><sub><em>k</em></sub></span> (weights) and <span class="math inline"><em>b</em></span> (bias) are the <strong>learned</strong> parameters (notice the lack of any superscript)</li>
<li><span class="math inline"><em>a</em><sup>(<em>i</em>)</sup></span> is the output of a non-linear activation function <span class="math inline"><em>g</em>(‚ãÖ)</span> applied to <span class="math inline"><em>z</em><sup>(<em>i</em>)</sup></span></li>
<li><span class="math inline"><em>yÃÇ</em><sup>(<em>i</em>)</sup></span> is the label we often give to the output (<span class="math inline"><em>a</em><sup>(<em>i</em>)</sup>‚ÄÑ=‚ÄÑ<em>yÃÇ</em><sup>(<em>i</em>)</sup></span>)</li>
</ul>
<p><strong>For this model, we want to find parameters <span class="math inline"><em>w</em><sub><em>k</em></sub></span> and <span class="math inline"><em>b</em></span> such that the neuron outputs <span class="math inline"><em>yÃÇ</em><sup>(<em>i</em>)</sup>‚ÄÑ‚âà‚ÄÑ<em>y</em></span> for any input.</strong> Before we discuss optimization we should take a moment to code up this single neuron model.</p>
<h2 id="code-for-computing-a-neurons-output">Code for Computing a Neuron‚Äôs Output</h2>
<p>This code does not containing any ‚Äúlearning‚Äù (i.e., optimization), but it is worth showing just how simple it is to write a single neuron from scratch. Nearly all code is used to create random input data.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">#!/usr/bin/env python</span></span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> math <span class="im">import</span> exp</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">from</span> random <span class="im">import</span> uniform</span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="kw">def</span> sigmoid(z: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb1-8"><a href="#cb1-8"></a>    <span class="co">&quot;&quot;&quot;The sigmoid/logistic activation function.&quot;&quot;&quot;</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> exp(<span class="op">-</span>z))</span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co"># The number of examples in our dataset</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>N <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb1-14"><a href="#cb1-14"></a></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co"># Randomly generate some input data</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>nx <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>x1 <span class="op">=</span> [uniform(<span class="op">-</span><span class="dv">20</span>, <span class="dv">40</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb1-18"><a href="#cb1-18"></a>x2 <span class="op">=</span> [uniform(<span class="dv">0</span>, <span class="fl">1e6</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb1-19"><a href="#cb1-19"></a>x3 <span class="op">=</span> [uniform(<span class="dv">0</span>, <span class="dv">24</span> <span class="op">*</span> <span class="dv">60</span> <span class="op">*</span> <span class="dv">60</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb1-20"><a href="#cb1-20"></a>x4 <span class="op">=</span> [<span class="bu">round</span>(uniform(<span class="dv">1</span>, <span class="dv">365</span>)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(N)]</span>
<span id="cb1-21"><a href="#cb1-21"></a></span>
<span id="cb1-22"><a href="#cb1-22"></a><span class="co"># Generate random neuron parameters</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>w1, w2, w3, w4 <span class="op">=</span> [uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(nx)]</span>
<span id="cb1-24"><a href="#cb1-24"></a>b <span class="op">=</span> uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-25"><a href="#cb1-25"></a></span>
<span id="cb1-26"><a href="#cb1-26"></a><span class="co"># Compute neuron output for each of the N examples</span></span>
<span id="cb1-27"><a href="#cb1-27"></a><span class="cf">for</span> x1i, x2i, x3i, x4i <span class="kw">in</span> <span class="bu">zip</span>(x1, x2, x3, x4):</span>
<span id="cb1-28"><a href="#cb1-28"></a>    zi <span class="op">=</span> w1 <span class="op">*</span> x1i <span class="op">+</span> w2 <span class="op">*</span> x2i <span class="op">+</span> w3 <span class="op">*</span> x3i <span class="op">+</span> w4 <span class="op">*</span> x4i <span class="op">+</span> b</span>
<span id="cb1-29"><a href="#cb1-29"></a>    ai <span class="op">=</span> sigmoid(zi)</span></code></pre></div>
<p>In the example, we have random parameters and we ignore the output. But what if we want to train the neuron so that the output mimics a real function or process? The next subsection tackles this very problem.</p>
<h2 id="optimization-with-batch-gradient-descent">Optimization with Batch Gradient Descent</h2>
<p>You may have noticed that in the previous code listing I also introduced a specific activation function (aka squashing function) called <code>sigmoid</code> (aka the logistic function). In this section we‚Äô</p>
<p>Do this first</p>
<p><strong>Deeper dive:</strong> TODO: something on activation functions.</p>
<h2 id="input-normalization">Input Normalization</h2>
<p>I provided <em>reasonable</em> ranges for values in the previous code example. For example, temperature values on Earth are typically in the range <span class="math inline">[‚àí20,40]</span> ¬∞C and illuminance in the range <span class="math inline">[0,1<em>e</em>6]</span> Lux.</p>
<p>An NN can work with with values in these ranges, but it makes learning easier when you first scale values into the same range, typically <span class="math inline">[‚àí1,1]</span>. TODO: why?</p>
<div class="sourceDiff">
<pre>


</pre>
</div>
<h2 id="parameter-initialization">Parameter Initialization</h2>
<!-- TODO: why can we start b at 0 by not w? -->
<h2 id="the-role-of-an-activation-function">The Role of an Activation Function</h2>
<ul>
<li>hidden neurons
<ul>
<li>default to relu</li>
<li>try/create others to solve/investigate specific issues</li>
</ul></li>
<li>output neurons
<ul>
<li>default to sigmoid for binary classification</li>
<li>default to softmax for multi-class classification</li>
<li>default to no activation for regression</li>
</ul></li>
</ul>
<h2 id="vectorization-with-pytorch">Vectorization with PyTorch</h2>
<h1 id="multi-layer-networks">Multi-Layer Networks</h1>
<ul>
<li>mlp</li>
<li>deep networks</li>
</ul>
<h1 id="gradient-descent">Gradient Descent</h1>
<h2 id="batch-gradient-descent">Batch Gradient Descent</h2>
<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>
<h2 id="mini-batch-stochastic-gradient-descent">Mini-Batch Stochastic Gradient Descent</h2>
<ul>
<li>SGD</li>
<li>Mini-Batch Stochastic Gradient Descent</li>
</ul>
<h1 id="optimization-techniques">Optimization Techniques</h1>
<h1 id="overfitting-and-generalization">Overfitting and Generalization</h1>
<h1 id="sec:cnns">Convolutional Neural Networks</h1>
<h1 id="recurrent-neural-networks">Recurrent Neural Networks</h1>
<p>recursive</p>
<h1 id="advanced-topics">Advanced Topics</h1>
<ul>
<li>Generative and Adversarial Training</li>
<li>NLP, Transformers, and Attention</li>
<li>Creating ML Applications</li>
<li>Reinforcement Learning</li>
<li>Hyperparameter Tuning</li>
<li>Hybrid Networks
<ul>
<li>complex inputs and outputs</li>
</ul></li>
<li>Alternative Machine Learning Techniques</li>
</ul>
