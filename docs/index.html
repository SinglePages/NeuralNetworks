<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Anthony J. Clark" />
  <title>Neural Networks</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="css/main.css">
  <style>
      body {
          font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
              Roboto, Oxygen-Sans, Ubuntu, Cantarell,
              "Helvetica Neue", sans-serif;
      }

      code {
          font-family: SFMono-Regular, Menlo, Monaco,
              Consolas, "Liberation Mono",
              "Courier New", monospace;
      }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Neural Networks</h1>
<p class="subtitle">A concise neural network walk-through</p>
<p class="author">Anthony J. Clark</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#sec:intro"><span class="toc-section-number">1</span>
Introduction</a></li>
<li><a href="#sec:ethics"><span class="toc-section-number">2</span>
Ethics</a></li>
<li><a href="#sec:data"><span class="toc-section-number">3</span>
Data</a></li>
<li><a href="#single-neuron"><span class="toc-section-number">4</span>
Single Neuron</a></li>
<li><a href="#neural-networks-and-backpropagation"><span
class="toc-section-number">5</span> Neural Networks and
Backpropagation</a></li>
<li><a href="#gradient-descent"><span
class="toc-section-number">6</span> Gradient Descent</a></li>
<li><a href="#sec:opti"><span class="toc-section-number">7</span>
Optimization Techniques</a></li>
<li><a href="#sec:generalization"><span
class="toc-section-number">8</span> Overfitting and
Generalization</a></li>
<li><a href="#sec:cnns"><span class="toc-section-number">9</span>
Convolutional Neural Networks</a></li>
<li><a href="#recurrent-neural-networks"><span
class="toc-section-number">10</span> Recurrent Neural Networks</a></li>
<li><a href="#attention-and-transformers"><span
class="toc-section-number">11</span> Attention and Transformers</a></li>
<li><a href="#sec:hyper"><span class="toc-section-number">12</span>
Advanced Topics</a></li>
<li><a href="#sec:terms">Terminology</a></li>
</ul>
</nav>
<p><span class="math display">\[
    \def\i{{^{(i)}}}
    \def\vx{{\mathbf{x}}}
    \def\vy{{\mathbf{y}}}
    \def\vw{{\mathbf{w}}}
    \def\vb{{\mathbf{b}}}
    \def\vz{{\mathbf{z}}}
    \def\va{{\mathbf{a}}}
    \def\yhat{{\hat y}}
    \def\vyhat{{\mathbf{\hat y}}}
    \def\mae{{||\vyhat - \vy||_1}}
    \def\vhmse{{\frac{1}{2N} ||(\vyhat - \vy)^2||_1}}
    \def\vbce{{-\frac{1}{N}\sum_{i=1}^N (y\i \log{\yhat\i} + (1 -
y\i)\log{(1-\yhat\i)})}}
\]</span></p>
<section id="sec:intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span>
Introduction</h1>
<blockquote>
<p>Any sufficiently advanced technology is indistinguishable from
magic.</p>
<p>– Arthur C. Clarke</p>
</blockquote>
<p>Goal: provide a concise walk-through of all fundamental neural
network (including modern deep learning) techniques.</p>
<p>I will not discuss every possible analogy, angle, or topic here.
Instead, I will provide links to external resources so that you can
choose which topics you want to investigate more closely. I will provide
minimal code examples when appropriate.</p>
<p><strong>Useful prior knowledge:</strong></p>
<ul>
<li>matrix calculus</li>
<li>programming skills</li>
<li>familiarity with computing tools</li>
</ul>
<section id="background" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span>
Background</h2>
<p>Artificial neurons date back to the 1940s and neural networks to the
1980s. These are not new techniques, but we surprisingly still have a
lot to learn about how they work and how to best create them. Research
into neural networks accelerated in the late 2000s as we found ourselves
with more data and more compute power.</p>
<p>Neural networks (NN) are a type of machine learning (ML) technique.
ML falls under the artificial intelligence (AI) umbrella. AI is a broad
area, and it doesn’t hurt to think of it as including any system that
appears to do something <em>useful</em> or <em>complex</em>.</p>
<p>You’ll find that techniques often start out as AI, but then we remove
that label after we start to better understand them. ML is just one type
of AI, and it comprises all techniques that automatically learn from
data. NNs learn from data, and specifically they do so using very little
input from the designer.</p>
<figure>
<img src="img/AI.svg"
alt="NNs are a subset of ML, which is a subset of AI." />
<figcaption aria-hidden="true">NNs are a subset of ML, which is a subset
of AI.</figcaption>
</figure>
<p>All of this is a bit vague, so let’s discuss some specific
applications. Maybe we want a NN to:</p>
<ul>
<li>Tell us if an Amazon review is positive or negative based on text
alone.</li>
<li>Tell us if an image contains a cat or a dog.</li>
<li>Translate an English sentence to German.</li>
<li>Tell us where in an image we can find a boat.</li>
<li>Automatically generate a caption for an image.</li>
<li>Direct a robot around a building.</li>
<li>Play a board game or a video game.</li>
<li>Tell us about the orientation of a persons limb’s for a virtual
reality game.</li>
<li>Prevent an autonomous car from driving off the road.</li>
<li>Group together all users of a social network that are likely to
listen to the same music.</li>
<li>Create a new piece of art.</li>
<li>Predict the sale price of a house.</li>
<li>Predict the future sale price of an investment.</li>
<li>Suggest products to purchase or movies to watch.</li>
<li>Diagnose an injury from an X-ray CT scan.</li>
<li>Automatically summarize a news article.</li>
<li>Label a news article as fake or real.</li>
</ul>
<p>This is just a small subset of what we could do. Nearly all
applications have the same basic flow:</p>
<figure>
<img src="img/MLProgram.svg" alt="General flow of data in a NN." />
<figcaption aria-hidden="true">General flow of data in a
NN.</figcaption>
</figure>
<p>The core of the NN is the ability to take and input, perform some
mathematical computations, and then produce the output. The “learning”
part includes comparing the output to a known-to-be-correct output (aka
the “label” or “target”) and then using this comparison to iteratively
improve the NN.</p>
<p>This setup, where we know the correct output, is known as “supervised
learning.” Later parts of this guide will touch on “unsupervised
learning” and “reinforcement learning,” but it is safe to say that most
ML applications are in the area of supervised learning.</p>
<details class="question">
<summary>
<strong>Question:</strong> What might be the input, output, label, and
criterion if we want an NN to distinguish between pictures of cats and
pictures of dogs?
</summary>
<div class="answer">
<strong>Answer:</strong> The input would be an image, the output would
be a guess of cat or dog, the label would be the actual contents of the
image, and the criterion should have something to do with if the output
guess was correct or not.
</div>
</details>
</section>
<section id="additional-material" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span>
Additional Material</h2>
<ul>
<li><a href="https://github.com/jakevdp/WhirlwindTourOfPython"
title="A Whirlwind Tour of Python">A Whirlwind Tour of Python (free
Book)</a></li>
<li><a href="https://explained.ai/matrix-calculus/">The Matrix Calculus
You Need For Deep Learning (web-page)</a></li>
<li><a href="https://d2l.ai/chapter_introduction/index.html"
title="Introduction — Dive into Deep Learning">Introduction — Dive into
Deep Learning (free Book)</a></li>
</ul>
</section>
</section>
<section id="sec:ethics" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span>
Ethics</h1>
<blockquote>
<p>A bit of the maker goes into that which they make.</p>
<p>– Unknown</p>
</blockquote>
<p>How is ethics important to NN (and AI in general)? It can help us
answer questions such as:</p>
<ul>
<li>What should we build?</li>
<li>What should we <strong>not</strong> build?</li>
<li>How should we build something?</li>
</ul>
<details class="question">
<summary>
<strong>Question:</strong> Who is in charge of enforcing ethics in AI?
</summary>
<div class="answer">
<strong>Answer:</strong> Everyone and no one. We do not have a special
<em>ethics force</em> to guide us. The problem is clearly that if
everyone is responsible, nobody will think they need to act.
</div>
</details>
<details class="question">
<summary>
<strong>Question:</strong> When should you start to consider ethical
implications?
</summary>
<div class="answer">
<p><strong>Answer:</strong> From the very beginning. This will make it
easier to:</p>
<ul>
<li>avoid pitfalls,</li>
<li>analyze results from an ethical lens,</li>
<li>avoid wasting time, and</li>
<li>ensure the system <strong>is</strong> ethical.</li>
</ul>
</div>
</details>
<p>Being ethical sounds like it should be easy, however we all come to
the table with our own value systems, opinions, motivations, and power.
What would you do if you were directed to build something you knew to be
unethical? How does your answer change if your choices are to build or
to quit?</p>
<section id="key-topics" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Key
Topics</h2>
<p>This is not going to be an exhaustive discussion on ethics in AI and
NNs. Instead, I’ll point you to the resources in the <a
href="#additional-material-1">Additional Material</a> section. The
topics below are taken from <a
href="https://github.com/fastai/fastbook/blob/master/03_ethics.ipynb"
title="fastbook/03_ethics.ipynb at master · fastai/fastbook">Ethics —
fastbook</a>.</p>
<p>Topics to consider:</p>
<ol type="1">
<li><strong>Recourse and accountability</strong>: who is responsible
(and liable) for the developed system? The user, developer, manager,
owner, company, other?</li>
<li><strong>Feedback loops</strong>: does the system control creation of
the next round of input data (such as a video recommendation
system)?</li>
<li><strong>Bias</strong>: all systems have bias; what bias is in your
system? Is the source of bias historical, from measurement, from
aggregation, from the representation, other?</li>
<li><strong>Disinformation</strong>: can your system be used for
nefarious goals?</li>
</ol>
</section>
<section id="strategies" class="level2" data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span>
Strategies</h2>
<p>Here are some questions you can ask to prevent running into
trouble:</p>
<ul>
<li><p>General Questions:</p>
<ul>
<li>Should we even be doing this?</li>
<li>What might be the accuracy of a simple non-ML alternative?</li>
<li>What processes will we use to handle appeals/mistakes?</li>
<li>How diverse is our team?</li>
</ul></li>
<li><p>Data Questions:</p>
<ul>
<li>Is our data valid for its intended use?</li>
<li>What bias could be in our data? (All data contains bias.)</li>
<li>How could we minimize bias in our data and model?</li>
<li>How should we “audit” our code and data?</li>
</ul></li>
<li><p>Impact Questions:</p>
<ul>
<li>Do we expect different errors rates for different sub-groups in the
data?</li>
<li>What are likely misinterpretations of the results and what can be
done to prevent those misinterpretations?</li>
<li>How might we impinge individuals’ privacy and/or anonymity?</li>
</ul></li>
</ul>
<p>When should you ask these questions? The Markkula Center for Applied
Ethics recommends scheduling regular meeting in which you perform
ethical risk sweeping. See their <a
href="https://www.scu.edu/ethics-in-technology-practice/ethical-toolkit/"
title="Ethical Toolkit - Markkula Center for Applied Ethics">Ethical
Toolkit</a> for more information.</p>
</section>
<section id="additional-material-1" class="level2" data-number="2.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span>
Additional Material</h2>
<ul>
<li><a href="https://ethics.fast.ai">Practical Data Ethics</a></li>
<li><a href="https://fairmlbook.org/">Fair ML Book</a></li>
<li><a href="https://www.machine-ethics.net/podcast/">Machine Ethics
Podcast</a></li>
<li>Codes of Ethics from the <a
href="https://www.acm.org/code-of-ethics">ACM</a>, <a
href="https://www.ieee.org/about/corporate/governance/p7-8.html">IEEE</a>,
and the <a
href="https://www.datascienceassn.org/code-of-conduct.html">Data Science
Association</a></li>
</ul>
</section>
</section>
<section id="sec:data" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span>
Data</h1>
<blockquote>
<p>Tidy datasets are all alike but every messy dataset is messy in its
own way.</p>
<p>– <a href="https://www.jstatsoft.org/article/view/v059i10/">Hadley
Wickham</a></p>
</blockquote>
<p>Perhaps the most important aspect of a neural network is the dataset.
Let</p>
<p><span class="math display">\[\mathcal{D} = \{X, Y\}\]</span></p>
<p>denote a dataset comprising input <em>features</em> <span
class="math inline">\(X\)</span> and output <em>targets</em> <span
class="math inline">\(Y\)</span>. Although <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> can come in many shapes, I am going to
be opinionated here and use a specific (and consistent) convention.
Let’s use <span class="math inline">\(N\)</span> to denote the size of
the paired dataset. (Note, not all problems have output targets, but
herein I am talking about supervised learning unless otherwise
specified.)</p>
<p>We will frequently take a dataset and split it into examples used for
training, validation, and evaluation. We’ll discuss these terms near the
end of this section.</p>
<p><span class="math inline">\(X\)</span> is a matrix (indicated by
capitalization) containing all features of all input examples. A single
input example <span class="math inline">\(\vx\i\)</span> is often
represented as a <em>column</em> vector (indicated by boldface):</p>
<p><span class="math display">\[\vx\i =
\begin{bmatrix}
    x\i_{1} \\
    x\i_{2} \\
    \vdots \\
    x\i_{n_x} \\
\end{bmatrix}
\]</span></p>
<p>where subscripts denote the feature index, <span
class="math inline">\(n_x\)</span> is the number of features, and the
superscript <span class="math inline">\(i\)</span> denotes that this is
the <span class="math inline">\(i^{\mathit{th}}\)</span> training
example. We do not always put the input features into a column vector
(see sec. 9 for more information), but it is fairly standard.</p>
<p>Each row in <span class="math inline">\(X\)</span> is a single input
example (also referred to as an instance or sample), and when you stack
all <span class="math inline">\(N\)</span> examples on top of each other
(first transposing them into row vectors), you end up with:</p>
<p><span class="math display">\[X =
\begin{bmatrix}
    \rule[.5ex]{1em}{0.4pt}\vx^{(1)T}\rule[.5ex]{1em}{0.4pt} \\
    \rule[.5ex]{1em}{0.4pt}\vx^{(2)T}\rule[.5ex]{1em}{0.4pt} \\
    \vdots \\
    \rule[.5ex]{1em}{0.4pt}\vx^{(N)T}\rule[.5ex]{1em}{0.4pt} \\
\end{bmatrix}
=
\begin{bmatrix}
    x^{(1)}_{1} &amp; x^{(1)}_{2} &amp; \cdots &amp;  x^{(1)}_{n_x} \\
    x^{(2)}_{1} &amp; x^{(2)}_{2} &amp; \cdots &amp;  x^{(2)}_{n_x} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
    x^{(N)}_{1} &amp; x^{(N)}_{2} &amp; \cdots &amp;  x^{(N)}_{n_x}
\end{bmatrix}
\]</span></p>
<p>We transpose each example column vector (i.e., <span
class="math inline">\(\vx^{(i)T}\)</span>) into a row vector so that the
first dimension of <span class="math inline">\(X\)</span> corresponds to
the number of examples <span class="math inline">\(N\)</span> and the
second dimension is the number of features <span
class="math inline">\(n_x\)</span>. Compare the column vector above to
each row in the matrix.</p>
<p>Let’s denote matrix dimensions with <span class="math inline">\((r
\times c)\)</span> (the number of rows <span
class="math inline">\(r\)</span> by the number of columns <span
class="math inline">\(c\)</span> in the matrix). I will, in text and in
code, refer to matrix dimensions as the “shape” of the matrix.</p>
<details class="question">
<summary>
<strong>Question:</strong> What is the shape of <span
class="math inline">\(X\)</span>?
</summary>
<div class="answer">
<strong>Answer:</strong> We say that <span class="math inline">\(\vx\i
\in \mathcal{R}^{n_x}\)</span> (each input example is <span
class="math inline">\(n_x\)</span> real values) and <span
class="math inline">\(X \in \mathcal{R}^{N \times n_x}\)</span>.
Therefore, the shape of <span class="math inline">\(X\)</span> is <span
class="math inline">\((N \times n_x)\)</span>.
</div>
</details>
<p><span class="math inline">\(Y\)</span> contains the targets (also
referred to as labels or the true/correct/actual/expected output
values). Here is a single target column vector:</p>
<p><span class="math display">\[\vy\i =
\begin{bmatrix}
    y\i_{1} \\
    y\i_{2} \\
    \vdots \\
    y\i_{n_y} \\
\end{bmatrix}
\]</span></p>
<p>And here is the entire target matrix including all examples:</p>
<p><span class="math display">\[Y =
\begin{bmatrix}
    \rule[.5ex]{1em}{0.4pt}\vy^{(1)T}\rule[.5ex]{1em}{0.4pt} \\
    \rule[.5ex]{1em}{0.4pt}\vy^{(2)T}\rule[.5ex]{1em}{0.4pt} \\
    \vdots \\
    \rule[.5ex]{1em}{0.4pt}\vy^{(N)T}\rule[.5ex]{1em}{0.4pt} \\
\end{bmatrix}
=
\begin{bmatrix}
    y^{(1)}_{1} &amp; y^{(1)}_{2} &amp; \cdots &amp;  y^{(1)}_{n_y} \\
    y^{(2)}_{1} &amp; y^{(2)}_{2} &amp; \cdots &amp;  y^{(2)}_{n_y} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
    y^{(N)}_{1} &amp; y^{(N)}_{2} &amp; \cdots &amp;  y^{(N)}_{n_y}
\end{bmatrix}
\]</span></p>
<details class="question">
<summary>
<strong>Question:</strong> What is the shape of <span
class="math inline">\(Y\)</span>?
</summary>
<div class="answer">
<strong>Answer:</strong> The shape of <span
class="math inline">\(Y\)</span> is <span class="math inline">\((N
\times n_y)\)</span>.
</div>
</details>
<p>Let’s use the <a
href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a> as
an example. This dataset comprises a training partition including 60,000
images and a validation partition including 10,000 images. Each image is
28 pixels in height and 28 pixels in width for a total of 784 pixels.
Each image depicts a single handwritten digit—a number in the range zero
through nine). Here is a small sample of these images:</p>
<figure>
<img
src="https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png"
alt="MNIST Sample. Image from Wikipedia." />
<figcaption aria-hidden="true">MNIST Sample. Image from
Wikipedia.</figcaption>
</figure>
<details class="question">
<summary>
<strong>Question:</strong> What is the shape of the training partition
of the input <span class="math inline">\(X_{train}\)</span>?
</summary>
<div class="answer">
<strong>Answer:</strong> <span class="math inline">\(X_{train}\)</span>
is <span class="math inline">\((60000 \times 784\)</span>): <span
class="math display">\[X =
\begin{bmatrix}
    x^{(1)}_{1} &amp; x^{(1)}_{2} &amp; \cdots &amp;  x^{(1)}_{784} \\
    x^{(2)}_{1} &amp; x^{(2)}_{2} &amp; \cdots &amp;  x^{(2)}_{784} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
    x^{(60000)}_{1} &amp; x^{(60000)}_{2} &amp; \cdots
&amp;  x^{(60000)}_{784}
\end{bmatrix}
\]</span> The first row includes all 784 pixels of the first training
image, and subsequent rows likewise contain pixel data for a single
image.
</div>
</details>
<details class="question">
<summary>
<strong>Question:</strong> What is the shape of the training partition
of the targets <span class="math inline">\(Y_{train}\)</span>?
</summary>
<div class="answer">
<strong>Answer:</strong> <span class="math inline">\(Y_{train}\)</span>
is <span class="math inline">\((60000 \times 10\)</span>): <span
class="math display">\[Y =
\begin{bmatrix}
    x^{(1)}_{1} &amp; x^{(1)}_{2} &amp; \cdots &amp;  x^{(1)}_{10} \\
    x^{(2)}_{1} &amp; x^{(2)}_{2} &amp; \cdots &amp;  x^{(2)}_{10} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
    x^{(60000)}_{1} &amp; x^{(60000)}_{2} &amp; \cdots
&amp;  x^{(60000)}_{10}
\end{bmatrix}
\]</span> Each row in this matrix is one-hot encoded, meaning that only
one item in each row is “1” and all other items in a row are “0”. Here
is an example of a one-hot encoding target for an input image
representing the digit “2” <span class="math display">\[y^T =
\begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;
0 &amp; 0 &amp; 0\end{bmatrix}\]</span> For efficiency sake, we often
represent a one-hot encoded vector using just the index of the “hot”
item. For example, the previous vector can be represented by the integer
2.
</div>
</details>
<details class="question">
<summary>
<strong>Question:</strong> What are the shapes of <span
class="math inline">\(X_{valid}\)</span> and <span
class="math inline">\(Y_{valid}\)</span>?
</summary>
<div class="answer">
<strong>Answer:</strong> <span class="math inline">\(X_{valid}\)</span>
and <span class="math inline">\(Y_{valid}\)</span> are <span
class="math inline">\((10000 \times 784)\)</span> and <span
class="math inline">\((10000 \times 10)\)</span>, respectively.
</div>
</details>
<p>You might now wonder why we split a dataset into
training/validation/evaluation partitions. It is reasonable to think
that we would be better off using all 70000 images to train a neural
network. However, we need some method for <em>measuring</em> how well a
model is performing. That is the purpose of the validation set–to
measure performance.</p>
<p>If we measure performance directly on the training dataset, we might
trick ourselves into thinking that the neural network will perform very
well when it is eventually deployed as part of an application (for
example, as a mobile app in which we convert an image of someones’s
handwritten notes into a text document), when in reality the network
might only perform well specifically on the examples found in the
training dataset. We will discuss this issue more in sec. 8 when we
cover overfitting and generalization.</p>
<p>Similarly, the evaluation partition is only used to compare
performance after hyper-parameter tuning, which we’ll discuss in
sec. 12.</p>
<section id="loading-mnist-using-pytorch" class="level2"
data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span>
Loading MNIST Using PyTorch</h2>
<p>We’ve discuss notation and general concepts, but how would we write
this out in code? Here is an example how how to load the MNIST dataset
using PyTorch.</p>
<div class="code-highlight">
<pre><span style="color:#39424e;">   1</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;">!/usr/bin/env python</span>
<span style="color:#39424e;">   2</span> 
<span style="color:#39424e;">   3</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">utils</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">data</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">DataLoader</span>
<span style="color:#39424e;">   4</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torchvision</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">datasets</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">MNIST</span>
<span style="color:#39424e;">   5</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torchvision</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">transforms</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">Compose</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">Normalize</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">ToTensor</span>
<span style="color:#39424e;">   6</span> 
<span style="color:#39424e;">   7</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Location in which to store downloaded data</span>
<span style="color:#39424e;">   8</span> <span style="color:#ccc9c2;">data_dir</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">../Data</span><span style="color:#bae67e;">&quot;</span>
<span style="color:#39424e;">   9</span> 
<span style="color:#39424e;">  10</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> I used torch.std_mean to find the values given to Normalize</span>
<span style="color:#39424e;">  11</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> We will discuss normalization in section 4</span>
<span style="color:#39424e;">  12</span> <span style="color:#ccc9c2;">mnist_xforms</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">Compose</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">[</span><span style="color:#ffd580;">ToTensor</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">Normalize</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">.</span><span style="color:#ffcc66;">1307</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">.</span><span style="color:#ffcc66;">3081</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">]</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  13</span> 
<span style="color:#39424e;">  14</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Load data files (training and validation partitions)</span>
<span style="color:#39424e;">  15</span> <span style="color:#ccc9c2;">train_data</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">MNIST</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">root</span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;">data_dir</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">train</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">True</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">download</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">True</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">transform</span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;">mnist_xforms</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  16</span> <span style="color:#ccc9c2;">valid_data</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">MNIST</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">root</span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;">data_dir</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">train</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">False</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">download</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">True</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">transform</span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;">mnist_xforms</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  17</span> 
<span style="color:#39424e;">  18</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Data loaders provide an easy interface for interactive with data</span>
<span style="color:#39424e;">  19</span> <span style="color:#ccc9c2;">train_loader</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">DataLoader</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">train_data</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">batch_size</span><span style="color:#f29e74;">=</span><span style="color:#f28779;">len</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">train_data</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  20</span> <span style="color:#ccc9c2;">valid_loader</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">DataLoader</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_data</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">batch_size</span><span style="color:#f29e74;">=</span><span style="color:#f28779;">len</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_data</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  21</span> 
<span style="color:#39424e;">  22</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> This odd bit of code forces the train loader to give us all inputs and targets</span>
<span style="color:#39424e;">  23</span> <span style="color:#ccc9c2;">X_train</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">y_train</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">next</span><span style="color:#ccc9c2;">(</span><span style="color:#f28779;">iter</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">train_loader</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  24</span> <span style="color:#ccc9c2;">X_valid</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">y_valid</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">next</span><span style="color:#ccc9c2;">(</span><span style="color:#f28779;">iter</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_loader</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  25</span> 
<span style="color:#39424e;">  26</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Let&apos;s start by simply printing out some basic information</span>
<span style="color:#39424e;">  27</span> <span style="color:#f28779;">print</span><span style="color:#ccc9c2;">(</span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">Training input shape    :</span><span style="color:#bae67e;">&quot;</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">X_train</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">shape</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  28</span> <span style="color:#f28779;">print</span><span style="color:#ccc9c2;">(</span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">Training target shape   :</span><span style="color:#bae67e;">&quot;</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">y_train</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">shape</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  29</span> <span style="color:#f28779;">print</span><span style="color:#ccc9c2;">(</span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">Validation input shape  :</span><span style="color:#bae67e;">&quot;</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">X_valid</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">shape</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  30</span> <span style="color:#f28779;">print</span><span style="color:#ccc9c2;">(</span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">Validation target shape :</span><span style="color:#bae67e;">&quot;</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">y_valid</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">shape</span><span style="color:#ccc9c2;">)</span>
</pre>
<div class="code-caption">
<a href="https://github.com/SinglePages/NeuralNetworks/blob/main/Source/Code/Python/03-01-LoadMNIST.py">Link
to code.</a>
</div>
</div>
<details class="question">
<summary>
<strong>Question:</strong> What do you expect to see as this program’s
output?
</summary>
<div class="answer">
<strong>Answer:</strong>
<pre class="code-block">
Training input shape    : torch.Size([60000, 1, 28, 28])
Training target shape   : torch.Size([60000])
Validation input shape  : torch.Size([10000, 1, 28, 28])
Validation target shape : torch.Size([10000])
</pre>
<p>This is slightly different than what we discussed. PyTorch expects us
to use this dataset with a convolutional neural network. When we get to
sec. 9 we’ll make more sense of this data format.</p>
</div>
</details>
</section>
<section id="similarity-digit-classifier" class="level2"
data-number="3.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span>
Similarity Digit Classifier</h2>
<p>Before we get into training NNs, we will start with a non-ML
classifier. This will provide a nice comparison, and show that ML must
be <em>learning</em> something beyond simple comparisons.</p>
<p>Let’s try to solve the following problem:</p>
<details class="question">
<summary>
<strong>Question:</strong> Given the MNIST dataset and also an image of
an unknown digit, how would you decide which digit is represented in the
unknown image?
</summary>
<div class="answer">
<strong>Answer:</strong> One method would be to find an “average” image
for the ten separate digits, and then compare the unknown image to the
ten averages and assign the unknown label as that of the closest average
image.
</div>
</details>
<p>For reference, here is what the “average” looks like for each of the
ten digits.</p>
<figure>
<img src="img/MNISTAverages.png"
alt="Average of the ten MNIST digits from the training dataset." />
<figcaption aria-hidden="true">Average of the ten MNIST digits from the
training dataset.</figcaption>
</figure>
<p>Before we show a solution, however, we should take a guess at how
well a random guesser might perform.</p>
<details class="question">
<summary>
<strong>Question:</strong> What percent of the time would you be correct
in guessing digits if you were guessing at random?
</summary>
<div class="answer">
<strong>Answer:</strong> If you are equally likely to guess any of the
ten digits, then you would be right around 10% of the time <span
class="math inline">\(\left(\frac{1}{10}\right)\)</span>. How might this
change if you were to always guess the same thing? How about if the
dataset has mostly ones and sevens?
</div>
</details>
<p>And now some code for finding the most similar digit.</p>
<div class="code-highlight">
<pre><span style="color:#39424e;">   1</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;">!/usr/bin/env python</span>
<span style="color:#39424e;">   2</span> 
<span style="color:#39424e;">   3</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">math</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">inf</span>
<span style="color:#39424e;">   4</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">matplotlib</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">pyplot</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">as</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">plt</span>
<span style="color:#39424e;">   5</span> 
<span style="color:#39424e;">   6</span> <span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span>
<span style="color:#39424e;">   7</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">utils</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">data</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">DataLoader</span>
<span style="color:#39424e;">   8</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torchvision</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">datasets</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">MNIST</span>
<span style="color:#39424e;">   9</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torchvision</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">transforms</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">Compose</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">Normalize</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">ToTensor</span>
<span style="color:#39424e;">  10</span> 
<span style="color:#39424e;">  11</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Location in which to store downloaded data</span>
<span style="color:#39424e;">  12</span> <span style="color:#ccc9c2;">data_dir</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">../Data</span><span style="color:#bae67e;">&quot;</span>
<span style="color:#39424e;">  13</span> 
<span style="color:#39424e;">  14</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> I used torch.std_mean to find the values given to Normalize</span>
<span style="color:#39424e;">  15</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> We will discuss normalization in section 4</span>
<span style="color:#39424e;">  16</span> <span style="color:#ccc9c2;">mnist_xforms</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">Compose</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">[</span><span style="color:#ffd580;">ToTensor</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">Normalize</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">.</span><span style="color:#ffcc66;">1307</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">.</span><span style="color:#ffcc66;">3081</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">]</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  17</span> 
<span style="color:#39424e;">  18</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Load data files (training and validation partitions)</span>
<span style="color:#39424e;">  19</span> <span style="color:#ccc9c2;">train_data</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">MNIST</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">root</span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;">data_dir</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">train</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">True</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">download</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">True</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">transform</span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;">mnist_xforms</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  20</span> <span style="color:#ccc9c2;">valid_data</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">MNIST</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">root</span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;">data_dir</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">train</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">False</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">download</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">True</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">transform</span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;">mnist_xforms</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  21</span> 
<span style="color:#39424e;">  22</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Data loaders provide an easy interface for interactive with data</span>
<span style="color:#39424e;">  23</span> <span style="color:#ccc9c2;">train_loader</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">DataLoader</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">train_data</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">batch_size</span><span style="color:#f29e74;">=</span><span style="color:#f28779;">len</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">train_data</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  24</span> <span style="color:#ccc9c2;">valid_loader</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">DataLoader</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_data</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">batch_size</span><span style="color:#f29e74;">=</span><span style="color:#f28779;">len</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_data</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  25</span> 
<span style="color:#39424e;">  26</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> This odd bit of code forces the train loader to give us all inputs and targets</span>
<span style="color:#39424e;">  27</span> <span style="color:#ccc9c2;">X_train</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">y_train</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">next</span><span style="color:#ccc9c2;">(</span><span style="color:#f28779;">iter</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">train_loader</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  28</span> <span style="color:#ccc9c2;">X_valid</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">y_valid</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">next</span><span style="color:#ccc9c2;">(</span><span style="color:#f28779;">iter</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_loader</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  29</span> 
<span style="color:#39424e;">  30</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Let&apos;s get the average for each digit based on all training examples</span>
<span style="color:#39424e;">  31</span> <span style="color:#ccc9c2;">digit_averages</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">{</span><span style="color:#ccc9c2;">}</span>
<span style="color:#39424e;">  32</span> <span style="color:#ffa759;">for</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">digit</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">in</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">range</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">10</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">:</span>
<span style="color:#39424e;">  33</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">digit_averages</span><span style="color:#ccc9c2;">[</span><span style="color:#ccc9c2;">digit</span><span style="color:#ccc9c2;">]</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">X_train</span><span style="color:#ccc9c2;">[</span><span style="color:#ccc9c2;">y_train</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">==</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">digit</span><span style="color:#ccc9c2;">]</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">mean</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">dim</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">)</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">squeeze</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  34</span> 
<span style="color:#39424e;">  35</span> 
<span style="color:#39424e;">  36</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Next up we need to compare &quot;unknown&quot; images to our average images</span>
<span style="color:#39424e;">  37</span> <span style="color:#ffa759;">def</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">get_most_similar</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">image</span><span style="color:#ccc9c2;">:</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">Tensor</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">averages</span><span style="color:#ccc9c2;">:</span><span style="color:#ccc9c2;"> </span><span style="color:#5ccfe6;">dict</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">:</span>
<span style="color:#39424e;">  38</span> <span style="color:#ccc9c2;">    </span><span style="color:#5c6773;">&quot;&quot;&quot;</span><span style="color:#5c6773;">Compare the image to each of the averaged images.</span>
<span style="color:#39424e;">  39</span> 
<span style="color:#39424e;">  40</span> <span style="color:#5c6773;">    Args:</span>
<span style="color:#39424e;">  41</span> <span style="color:#5c6773;">        image (torch.Tensor): an image represented as a tensor</span>
<span style="color:#39424e;">  42</span> <span style="color:#5c6773;">        averages (dict): a dictionary of averaged images</span>
<span style="color:#39424e;">  43</span> 
<span style="color:#39424e;">  44</span> <span style="color:#5c6773;">    Returns:</span>
<span style="color:#39424e;">  45</span> <span style="color:#5c6773;">        the most similar label</span>
<span style="color:#39424e;">  46</span> <span style="color:#5c6773;">    </span><span style="color:#5c6773;">&quot;&quot;&quot;</span>
<span style="color:#39424e;">  47</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">closest_label</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">None</span>
<span style="color:#39424e;">  48</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">closest_distance</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">inf</span>
<span style="color:#39424e;">  49</span> <span style="color:#ccc9c2;">    </span><span style="color:#ffa759;">for</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">label</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">in</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">averages</span><span style="color:#ccc9c2;">:</span>
<span style="color:#39424e;">  50</span> <span style="color:#ccc9c2;">        </span><span style="color:#ccc9c2;">distance</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">image</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">averages</span><span style="color:#ccc9c2;">[</span><span style="color:#ccc9c2;">label</span><span style="color:#ccc9c2;">]</span><span style="color:#ccc9c2;">)</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">abs</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">mean</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  51</span> <span style="color:#ccc9c2;">        </span><span style="color:#ffa759;">if</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">distance</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">&lt;</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">closest_distance</span><span style="color:#ccc9c2;">:</span>
<span style="color:#39424e;">  52</span> <span style="color:#ccc9c2;">            </span><span style="color:#ccc9c2;">closest_label</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">label</span>
<span style="color:#39424e;">  53</span> <span style="color:#ccc9c2;">            </span><span style="color:#ccc9c2;">closest_distance</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">distance</span>
<span style="color:#39424e;">  54</span> <span style="color:#ccc9c2;">    </span><span style="color:#ffa759;">return</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">closest_label</span>
<span style="color:#39424e;">  55</span> 
<span style="color:#39424e;">  56</span> 
<span style="color:#39424e;">  57</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Now we can get the most similar label for each validation image</span>
<span style="color:#39424e;">  58</span> <span style="color:#ccc9c2;">num_correct</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">0</span>
<span style="color:#39424e;">  59</span> <span style="color:#ffa759;">for</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">image</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">label</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">in</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">zip</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">X_valid</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">y_valid</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">:</span>
<span style="color:#39424e;">  60</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">num_correct</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">label</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">==</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">get_most_similar</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">image</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">digit_averages</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  61</span> 
<span style="color:#39424e;">  62</span> <span style="color:#f28779;">print</span><span style="color:#ccc9c2;">(</span><span style="color:#ffa759;">f</span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">Percent guessed correctly: </span><span style="color:#ccc9c2;">{</span><span style="color:#ccc9c2;">num_correct</span><span style="color:#f29e74;">/</span><span style="color:#f28779;">len</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">X_valid</span><span style="color:#ccc9c2;">)</span><span style="color:#f29e74;">*</span><span style="color:#ffcc66;">100</span><span style="color:#ffcc66;">:.2f</span><span style="color:#ccc9c2;">}</span><span style="color:#bae67e;">%</span><span style="color:#bae67e;">&quot;</span><span style="color:#ccc9c2;">)</span>
</pre>
<div class="code-caption">
<a href="https://github.com/SinglePages/NeuralNetworks/blob/main/Source/Code/Python/03-02-MNISTSimilarity.py">Link
to code.</a>
</div>
</div>
<details class="question">
<summary>
<strong>Question:</strong> Take a guess at the accuracy our
similarity-based model.
</summary>
<div class="answer">
<strong>Answer:</strong> This model is correct about 66.85% of the time.
</div>
</details>
</section>
</section>
<section id="single-neuron" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Single
Neuron</h1>
<blockquote>
<p>A single neuron in the brain is an incredibly complex machine that
even today we don’t understand. A single “neuron” in a neural network is
an incredibly simple mathematical function that captures a minuscule
fraction of the complexity of a biological neuron.</p>
<p>– <a
href="https://www.wired.com/2015/02/google-brains-co-inventor-tells-why-hes-building-chinese-neural-networks/">Andrew
Ng</a></p>
</blockquote>
<p>When our model is a single neuron we can only produce a single
output. So, <span class="math inline">\(n_y=1\)</span> for this section.
Sticking to our MNSIT digits example from above, we could train a single
neuron to distinguish between two different classes of digits (e.g., “1”
vs “7”, “0” vs “non-zero”, etc.).</p>
<section id="notation-and-diagram" class="level2" data-number="4.1">
<h2 data-number="4.1"><span class="header-section-number">4.1</span>
Notation and Diagram</h2>
<p>Here is a diagram representing a single neuron (as we’ll see later,
some neural networks are just many of these neurons interconnected):</p>
<figure>
<img src="img/NeuronSeparate.svg"
alt="A neuron model with separate nodes for linear and activation computations." />
<figcaption aria-hidden="true">A neuron model with separate nodes for
linear and activation computations.</figcaption>
</figure>
<p>The diagram represents the following equations (note that I removed
the parenthesis superscript from the diagram to make it a bit easier to
read):</p>
<p><span class="math display">\[\begin{align}
z\i &amp;= \sum_{k=1}^{n_x} x_k\i w_k + b\\
a\i &amp;= g(z\i)
\end{align}\]</span></p>
<p>For these two equations:</p>
<ul>
<li><span class="math inline">\(x_k\i\)</span> are the input features
for the <span class="math inline">\(i^{th}\)</span> example (e.g., <span
class="math inline">\(k=76\)</span> and <span
class="math inline">\(i=7436\)</span> would denote pixel 76 of 784 for
image 7436 of 60000)</li>
<li><span class="math inline">\(w_k\)</span> (weights) and <span
class="math inline">\(b\)</span> (bias) are the <strong>learned</strong>
parameters</li>
<li><span class="math inline">\(z\i\)</span> is a weighted sum of the
input features plus the additional bias term</li>
<li><span class="math inline">\(a\i\)</span> is the output of a
non-linear activation function <span
class="math inline">\(g(\mathord{\cdot})\)</span> applied to <span
class="math inline">\(z\i\)</span></li>
<li><span class="math inline">\(\yhat\i\)</span> (pronounced <em>“y
hat”</em>) is the label we often give to the output (<span
class="math inline">\(a\i = \yhat\i\)</span>)</li>
</ul>
<details class="question">
<summary>
<strong>Question:</strong> Why do <span
class="math inline">\(w_k\)</span> and <span
class="math inline">\(b\)</span> not have superscripts?
</summary>
<div class="answer">
<strong>Answer:</strong> The parameters <span
class="math inline">\(w_k\)</span> and <span
class="math inline">\(b\)</span> do not change as the input <span
class="math inline">\(x_k\i\)</span> changes. These parameters
<strong>are</strong> the neuron, and they are used to produce the output
<span class="math inline">\(\yhat\i\)</span> for any given input; we use
the same parameter values regardless of input.
</div>
</details>
<p><strong>For this model, we want to find parameters <span
class="math inline">\(w_k\)</span> and <span
class="math inline">\(b\)</span> such that the neuron outputs <span
class="math inline">\(\yhat\i \approx y\i\)</span> for any
input.</strong> Before we discuss optimization we should take a moment
to code up this single neuron model.</p>
<p>(Below is a more common representation of a neuron model. The image
above separates the linear and activation components into distinct
nodes, but it is more common to show them together as below.)</p>
<figure>
<img src="img/Neuron.svg" alt="A neuron model." />
<figcaption aria-hidden="true">A neuron model.</figcaption>
</figure>
</section>
<section id="neuron-with-python-standard-libraries" class="level2"
data-number="4.2">
<h2 data-number="4.2"><span class="header-section-number">4.2</span>
Neuron with Python Standard Libraries</h2>
<p>This code does <strong>not</strong> include any “learning” (i.e.,
optimization), but it is worth showing just how simple it is to write a
single neuron from scratch. Most of the code below is necessary only to
create some faked input data.</p>
<div class="code-highlight">
<pre><span style="color:#39424e;">   1</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;">!/usr/bin/env python</span>
<span style="color:#39424e;">   2</span> 
<span style="color:#39424e;">   3</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">math</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">exp</span>
<span style="color:#39424e;">   4</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">random</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">gauss</span>
<span style="color:#39424e;">   5</span> 
<span style="color:#39424e;">   6</span> 
<span style="color:#39424e;">   7</span> <span style="color:#ffa759;">def</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">sigmoid</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">z</span><span style="color:#ccc9c2;">:</span><span style="color:#ccc9c2;"> </span><span style="color:#5ccfe6;">float</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">-&gt;</span><span style="color:#ccc9c2;"> </span><span style="color:#5ccfe6;">float</span><span style="color:#ccc9c2;">:</span>
<span style="color:#39424e;">   8</span> <span style="color:#ccc9c2;">    </span><span style="color:#5c6773;">&quot;&quot;&quot;</span><span style="color:#5c6773;">The sigmoid/logistic activation function.</span><span style="color:#5c6773;">&quot;&quot;&quot;</span>
<span style="color:#39424e;">   9</span> <span style="color:#ccc9c2;">    </span><span style="color:#ffa759;">return</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">/</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">exp</span><span style="color:#ccc9c2;">(</span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;">z</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  10</span> 
<span style="color:#39424e;">  11</span> 
<span style="color:#39424e;">  12</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> The number of examples in our dataset</span>
<span style="color:#39424e;">  13</span> <span style="color:#ccc9c2;">N</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">100</span>
<span style="color:#39424e;">  14</span> 
<span style="color:#39424e;">  15</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Randomly generate some input data</span>
<span style="color:#39424e;">  16</span> <span style="color:#ccc9c2;">nx</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">4</span>
<span style="color:#39424e;">  17</span> <span style="color:#ccc9c2;">x1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">[</span><span style="color:#ffd580;">gauss</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">for</span><span style="color:#ccc9c2;"> </span><span style="color:#5ccfe6;">_</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">in</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">range</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">N</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">]</span>
<span style="color:#39424e;">  18</span> <span style="color:#ccc9c2;">x2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">[</span><span style="color:#ffd580;">gauss</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">for</span><span style="color:#ccc9c2;"> </span><span style="color:#5ccfe6;">_</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">in</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">range</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">N</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">]</span>
<span style="color:#39424e;">  19</span> <span style="color:#ccc9c2;">x3</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">[</span><span style="color:#ffd580;">gauss</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">for</span><span style="color:#ccc9c2;"> </span><span style="color:#5ccfe6;">_</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">in</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">range</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">N</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">]</span>
<span style="color:#39424e;">  20</span> <span style="color:#ccc9c2;">x4</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">[</span><span style="color:#ffd580;">gauss</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">for</span><span style="color:#ccc9c2;"> </span><span style="color:#5ccfe6;">_</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">in</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">range</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">N</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">]</span>
<span style="color:#39424e;">  21</span> 
<span style="color:#39424e;">  22</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Generate random neuron parameters</span>
<span style="color:#39424e;">  23</span> <span style="color:#ccc9c2;">w1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">gauss</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  24</span> <span style="color:#ccc9c2;">w2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">gauss</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  25</span> <span style="color:#ccc9c2;">w3</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">gauss</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  26</span> <span style="color:#ccc9c2;">w4</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">gauss</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  27</span> <span style="color:#ccc9c2;">b</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">0</span>
<span style="color:#39424e;">  28</span> 
<span style="color:#39424e;">  29</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Compute neuron output for each of the N examples</span>
<span style="color:#39424e;">  30</span> <span style="color:#ffa759;">for</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">x1i</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">x2i</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">x3i</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">x4i</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">in</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">zip</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">x1</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">x2</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">x3</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">x4</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">:</span>
<span style="color:#39424e;">  31</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">zi</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">w1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">x1i</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">w2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">x2i</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">w3</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">x3i</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">w4</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">x4i</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">b</span>
<span style="color:#39424e;">  32</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">ai</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">sigmoid</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">zi</span><span style="color:#ccc9c2;">)</span>
</pre>
<div class="code-caption">
<a href="https://github.com/SinglePages/NeuralNetworks/blob/main/Source/Code/Python/04-01-NeuronLoop.py">Link
to code.</a>
</div>
</div>
<p>In this code listing I use the <code>sigmoid</code> activation
function (when not using a specific activation function we use <span
class="math inline">\(g(\mathord{\cdot})\)</span> in most equations).
This function is plotted below.</p>
<figure>
<img src="img/Sigmoid.png"
alt="Sigmoid activation function and its derivative." />
<figcaption aria-hidden="true">Sigmoid activation function and its
derivative.</figcaption>
</figure>
<p>Some nice properties of this function include:</p>
<ul>
<li>An output range of [0, 1] (all inputs are “squashed” into this
range).</li>
<li>An easy to compute derivative.</li>
<li>Easy to interpret and understand.</li>
<li>Well-known.</li>
</ul>
<p>We often use sigmoid activation functions for binary classification
(i.e., models trained to predict whether an input belongs to one of two
classes). If the output is <span class="math inline">\(≤0.5\)</span> we
say the neuron predicts class <span class="math inline">\(A\)</span>
otherwise class <span class="math inline">\(B\)</span>.</p>
<details class="question">
<summary>
<strong>Question:</strong> Can you think of any downsides for this
function (hint: look at the derivative curve)?
</summary>
<div class="answer">
<strong>Answer:</strong> While this function was once widely used, it
has fallen out of favor because it can often lead to slower learning due
to small derivative values for any input <span
class="math inline">\(z\)</span> outside of the range [-4, 4]. <a
href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a>
is a more commonly used activation function for hidden layer neurons.
</div>
</details>
</section>
<section id="the-dot-product" class="level2" data-number="4.3">
<h2 data-number="4.3"><span class="header-section-number">4.3</span> The
Dot-Product</h2>
<p>We compute <span class="math inline">\(z\i\)</span> using a
summation, but we can express this same bit of math using the
dot-product from linear algebra.</p>
<p><span class="math display">\[
z\i = \sum_{k=1}^{n_x} x_k\i w_k + b = \vx^{(i)T} \vw + b
\]</span></p>
<p>The <span class="math inline">\(\vx^{(i)T} \vw\)</span> part of the
equation computes the dot-product between <span
class="math inline">\(\vx^{(i)T}\)</span> and <span
class="math inline">\(\vw\)</span>. We need to transpose <span
class="math inline">\(\vx\i\)</span> to make the dimensions work (i.e.,
we need to multiply a row vector by a column vector).</p>
<p>This not only turns out to be easier to write/type, but it is more
efficiently computed by a neural network library. The code listing below
uses <a href="https://pytorch.org/">PyTorch</a> to compute <span
class="math inline">\(z\i\)</span> (<code>zi</code>). Libraries like
PyTorch and Tensorflow make use of both vectorized CPU instructions and
graphics cards (GPUs) to quickly compute the output of matrix
multiplications.</p>
<div class="code-highlight">
<pre>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 1  </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 1  </span><span style="color:#0000ee;">│</span><span style="color:#75715e;">#!/usr/bin/env python</span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 2  </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 2  </span><span style="color:#0000ee;">│</span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 3  </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f92672;background-color:#a61719;">from</span><span style="color:#f8f8f2;background-color:#a61719;"> math </span><span style="color:#f92672;background-color:#a61719;">import</span><span style="color:#f8f8f2;background-color:#a61719;"> exp</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 4  </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f92672;background-color:#a61719;">from</span><span style="color:#f8f8f2;background-color:#a61719;"> random </span><span style="color:#f92672;background-color:#a61719;">import</span><span style="color:#f8f8f2;background-color:#a61719;"> gauss</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 5  </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 6  </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 7  </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f92672;background-color:#a61719;">def</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#a6e22e;background-color:#a61719;">sigmoid</span><span style="color:#f8f8f2;background-color:#a61719;">(</span><span style="color:#fd971f;background-color:#a61719;">z</span><span style="color:#f8f8f2;background-color:#a61719;">: </span><span style="color:#a6e22e;background-color:#a61719;">float</span><span style="color:#f8f8f2;background-color:#a61719;">) -&gt; </span><span style="color:#a6e22e;background-color:#a61719;">float</span><span style="color:#f8f8f2;background-color:#a61719;">:</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 8  </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">    </span><span style="color:#75715e;background-color:#a61719;">&quot;&quot;&quot;The sigmoid/logistic activation function.&quot;&quot;&quot;</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 9  </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">    </span><span style="color:#f92672;background-color:#a61719;">return</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#be84ff;background-color:#a61719;">1</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#f92672;background-color:#a61719;">/</span><span style="color:#f8f8f2;background-color:#a61719;"> (</span><span style="color:#be84ff;background-color:#a61719;">1</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#f92672;background-color:#a61719;">+</span><span style="color:#f8f8f2;background-color:#a61719;"> exp(</span><span style="color:#f92672;background-color:#a61719;">-</span><span style="color:#f8f8f2;background-color:#a61719;">z))</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;">    </span><span style="color:#0000ee;">│</span><span style="color:#008700;"> 3  </span><span style="color:#0000ee;">│</span><span style="color:#f92672;background-color:#22a322;">import</span><span style="color:#f8f8f2;background-color:#22a322;"> torch</span><span style="background-color:#22a322;"></span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 10 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 4  </span><span style="color:#0000ee;">│</span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 11 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 5  </span><span style="color:#0000ee;">│</span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 12 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 6  </span><span style="color:#0000ee;">│</span><span style="color:#75715e;"># The number of examples in our dataset</span>

<span style="color:#0000ee;"></span><span style="color:#444444;"> 14 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 8  </span><span style="color:#0000ee;">│</span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 15 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 9  </span><span style="color:#0000ee;">│</span><span style="color:#75715e;"># Randomly generate some input data</span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 16 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 10 </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;">nx </span><span style="color:#f92672;">=</span><span style="color:#f8f8f2;"> </span><span style="color:#be84ff;">4</span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 17 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">x1 </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> [gauss(</span><span style="color:#be84ff;background-color:#a61719;">0</span><span style="color:#f8f8f2;background-color:#a61719;">, </span><span style="color:#be84ff;background-color:#a61719;">1</span><span style="color:#f8f8f2;background-color:#a61719;">) </span><span style="color:#f92672;background-color:#a61719;">for</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#ffffff;background-color:#a61719;">_</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#f92672;background-color:#a61719;">in</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#66d9ef;background-color:#a61719;">range</span><span style="color:#f8f8f2;background-color:#a61719;">(N)]</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 18 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">x2 </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> [gauss(</span><span style="color:#be84ff;background-color:#a61719;">0</span><span style="color:#f8f8f2;background-color:#a61719;">, </span><span style="color:#be84ff;background-color:#a61719;">1</span><span style="color:#f8f8f2;background-color:#a61719;">) </span><span style="color:#f92672;background-color:#a61719;">for</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#ffffff;background-color:#a61719;">_</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#f92672;background-color:#a61719;">in</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#66d9ef;background-color:#a61719;">range</span><span style="color:#f8f8f2;background-color:#a61719;">(N)]</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 19 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">x3 </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> [gauss(</span><span style="color:#be84ff;background-color:#a61719;">0</span><span style="color:#f8f8f2;background-color:#a61719;">, </span><span style="color:#be84ff;background-color:#a61719;">1</span><span style="color:#f8f8f2;background-color:#a61719;">) </span><span style="color:#f92672;background-color:#a61719;">for</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#ffffff;background-color:#a61719;">_</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#f92672;background-color:#a61719;">in</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#66d9ef;background-color:#a61719;">range</span><span style="color:#f8f8f2;background-color:#a61719;">(N)]</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 20 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">x4 </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> [gauss(</span><span style="color:#be84ff;background-color:#a61719;">0</span><span style="color:#f8f8f2;background-color:#a61719;">, </span><span style="color:#be84ff;background-color:#a61719;">1</span><span style="color:#f8f8f2;background-color:#a61719;">) </span><span style="color:#f92672;background-color:#a61719;">for</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#ffffff;background-color:#a61719;">_</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#f92672;background-color:#a61719;">in</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#66d9ef;background-color:#a61719;">range</span><span style="color:#f8f8f2;background-color:#a61719;">(N)]</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;">    </span><span style="color:#0000ee;">│</span><span style="color:#008700;"> 11 </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#22a322;">X </span><span style="color:#f92672;background-color:#22a322;">=</span><span style="color:#f8f8f2;background-color:#22a322;"> torch.randn(N, nx)</span><span style="background-color:#22a322;"></span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 21 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 12 </span><span style="color:#0000ee;">│</span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 22 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 13 </span><span style="color:#0000ee;">│</span><span style="color:#75715e;"># Generate random neuron parameters</span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 23 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">w1 </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> gauss(</span><span style="color:#be84ff;background-color:#a61719;">0</span><span style="color:#f8f8f2;background-color:#a61719;">, </span><span style="color:#be84ff;background-color:#a61719;">1</span><span style="color:#f8f8f2;background-color:#a61719;">)</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 24 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">w2 </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> gauss(</span><span style="color:#be84ff;background-color:#a61719;">0</span><span style="color:#f8f8f2;background-color:#a61719;">, </span><span style="color:#be84ff;background-color:#a61719;">1</span><span style="color:#f8f8f2;background-color:#a61719;">)</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 25 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">w3 </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> gauss(</span><span style="color:#be84ff;background-color:#a61719;">0</span><span style="color:#f8f8f2;background-color:#a61719;">, </span><span style="color:#be84ff;background-color:#a61719;">1</span><span style="color:#f8f8f2;background-color:#a61719;">)</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 26 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">w4 </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> gauss(</span><span style="color:#be84ff;background-color:#a61719;">0</span><span style="color:#f8f8f2;background-color:#a61719;">, </span><span style="color:#be84ff;background-color:#a61719;">1</span><span style="color:#f8f8f2;background-color:#a61719;">)</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;">    </span><span style="color:#0000ee;">│</span><span style="color:#008700;"> 14 </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#22a322;">w </span><span style="color:#f92672;background-color:#22a322;">=</span><span style="color:#f8f8f2;background-color:#22a322;"> torch.randn(nx)</span><span style="background-color:#22a322;"></span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 27 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 15 </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;">b </span><span style="color:#f92672;">=</span><span style="color:#f8f8f2;"> </span><span style="color:#be84ff;">0</span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 28 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 16 </span><span style="color:#0000ee;">│</span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 29 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 17 </span><span style="color:#0000ee;">│</span><span style="color:#75715e;"># Compute neuron output for each of the N examples</span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 30 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f92672;background-color:#a61719;">for</span><span style="color:#f8f8f2;background-color:#a61719;"> x1i, x2i, x3i, x4i </span><span style="color:#f92672;background-color:#a61719;">in</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#66d9ef;background-color:#a61719;">zip</span><span style="color:#f8f8f2;background-color:#a61719;">(x1, x2, x3, x4):</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 31 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">    zi </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> w1 </span><span style="color:#f92672;background-color:#a61719;">*</span><span style="color:#f8f8f2;background-color:#a61719;"> x1i </span><span style="color:#f92672;background-color:#a61719;">+</span><span style="color:#f8f8f2;background-color:#a61719;"> w2 </span><span style="color:#f92672;background-color:#a61719;">*</span><span style="color:#f8f8f2;background-color:#a61719;"> x2i </span><span style="color:#f92672;background-color:#a61719;">+</span><span style="color:#f8f8f2;background-color:#a61719;"> w3 </span><span style="color:#f92672;background-color:#a61719;">*</span><span style="color:#f8f8f2;background-color:#a61719;"> x3i </span><span style="color:#f92672;background-color:#a61719;">+</span><span style="color:#f8f8f2;background-color:#a61719;"> w4 </span><span style="color:#f92672;background-color:#a61719;">*</span><span style="color:#f8f8f2;background-color:#a61719;"> x4i </span><span style="color:#f92672;background-color:#a61719;">+</span><span style="color:#f8f8f2;background-color:#a61719;"> b</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 32 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#a61719;">    ai </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> sigmoid(zi)</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;">    </span><span style="color:#0000ee;">│</span><span style="color:#008700;"> 18 </span><span style="color:#0000ee;">│</span><span style="color:#f92672;background-color:#22a322;">for</span><span style="color:#f8f8f2;background-color:#22a322;"> xi </span><span style="color:#f92672;background-color:#22a322;">in</span><span style="color:#f8f8f2;background-color:#22a322;"> X:</span><span style="background-color:#22a322;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;">    </span><span style="color:#0000ee;">│</span><span style="color:#008700;"> 19 </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#22a322;">    zi </span><span style="color:#f92672;background-color:#22a322;">=</span><span style="color:#f8f8f2;background-color:#22a322;"> xi </span><span style="color:#f92672;background-color:#22a322;">&#64;</span><span style="color:#f8f8f2;background-color:#22a322;"> w </span><span style="color:#f92672;background-color:#22a322;">+</span><span style="color:#f8f8f2;background-color:#22a322;"> b</span><span style="background-color:#22a322;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;">    </span><span style="color:#0000ee;">│</span><span style="color:#008700;"> 20 </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;background-color:#22a322;">    ai </span><span style="color:#f92672;background-color:#22a322;">=</span><span style="color:#f8f8f2;background-color:#22a322;"> </span><span style="font-weight:bold;color:#7f7f7f;background-color:#006000;">torch.</span><span style="color:#f8f8f2;background-color:#22a322;">sigmoid(zi)</span><span style="background-color:#22a322;"></span>
</pre>
<div class="code-caption">
<a href="https://github.com/SinglePages/NeuralNetworks/blob/main/Source/Code/Python/04-02-NeuronDot.py">Link
to code.</a>
</div>
</div>
<p>The code snippet above shows a <a
href="https://en.wikipedia.org/wiki/Diff">diff</a> between the previous
code snippet and an updated one using the dot product. You will see many
diffs throughout this document. The key points are that: (1) red
indicates text or entire lines that have been removed and (2) green
indicates updated or newly added lines.</p>
<p>We do not need to transpose <code>xi</code> in code because when we
iteration through <code>X</code> we get row vectors. As it happens, we
can improve efficiency even further.</p>
</section>
<section id="vectorizing-inputs" class="level2" data-number="4.4">
<h2 data-number="4.4"><span class="header-section-number">4.4</span>
Vectorizing Inputs</h2>
<p>In addition to using a dot-product in place of a summation, we can
use a matrix multiplication in place of looping over all examples in the
dataset. In the two equations below we perform a matrix multiplication
that computes the output of the network for all examples at once. A
neural network library can turn this into highly efficient CPU or GPU
operations.</p>
<p><span class="math display">\[\begin{align}
\vz &amp;= X \vw + \mathbf{1} b \\
\va &amp;= g(\vz)
\end{align}\]</span></p>
<div class="code-highlight">
<pre>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 15 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 15 </span><span style="color:#0000ee;">│</span><span style="color:#f8f8f2;">b </span><span style="color:#f92672;">=</span><span style="color:#f8f8f2;"> </span><span style="color:#be84ff;">0</span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 16 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 16 </span><span style="color:#0000ee;">│</span>
<span style="color:#0000ee;"></span><span style="color:#444444;"> 17 </span><span style="color:#0000ee;">│</span><span style="color:#444444;"> 17 </span><span style="color:#0000ee;">│</span><span style="color:#75715e;"># Compute neuron output for each of the N examples</span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 18 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="color:#f92672;background-color:#a61719;">for</span><span style="color:#f8f8f2;background-color:#a61719;"> xi </span><span style="color:#f92672;background-color:#a61719;">in</span><span style="color:#f8f8f2;background-color:#a61719;"> X:</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 19 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="font-weight:bold;color:#f8f8f2;background-color:#901011;">    zi </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="font-weight:bold;color:#7f7f7f;background-color:#901011;">xi</span><span style="color:#f8f8f2;background-color:#a61719;"> </span><span style="color:#f92672;background-color:#a61719;">&#64;</span><span style="color:#f8f8f2;background-color:#a61719;"> w </span><span style="color:#f92672;background-color:#a61719;">+</span><span style="color:#f8f8f2;background-color:#a61719;"> b</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;"> 20 </span><span style="color:#0000ee;">│</span><span style="color:#008700;">    </span><span style="color:#0000ee;">│</span><span style="font-weight:bold;color:#f8f8f2;background-color:#901011;">    ai </span><span style="color:#f92672;background-color:#a61719;">=</span><span style="color:#f8f8f2;background-color:#a61719;"> torch.sigmoid(</span><span style="font-weight:bold;color:#7f7f7f;background-color:#901011;">zi</span><span style="color:#f8f8f2;background-color:#a61719;">)</span><span style="background-color:#a61719;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;">    </span><span style="color:#0000ee;">│</span><span style="color:#008700;"> 18 </span><span style="color:#0000ee;">│</span><span style="font-weight:bold;color:#f8f8f2;background-color:#006000;">z </span><span style="color:#f92672;background-color:#22a322;">=</span><span style="color:#f8f8f2;background-color:#22a322;"> </span><span style="font-weight:bold;color:#7f7f7f;background-color:#006000;">X</span><span style="color:#f8f8f2;background-color:#22a322;"> </span><span style="color:#f92672;background-color:#22a322;">&#64;</span><span style="color:#f8f8f2;background-color:#22a322;"> w </span><span style="color:#f92672;background-color:#22a322;">+</span><span style="color:#f8f8f2;background-color:#22a322;"> b</span><span style="background-color:#22a322;"></span>
<span style="color:#0000ee;"></span><span style="color:#870000;">    </span><span style="color:#0000ee;">│</span><span style="color:#008700;"> 19 </span><span style="color:#0000ee;">│</span><span style="font-weight:bold;color:#f8f8f2;background-color:#006000;">yhat </span><span style="color:#f92672;background-color:#22a322;">=</span><span style="color:#f8f8f2;background-color:#22a322;"> torch.sigmoid(</span><span style="font-weight:bold;color:#7f7f7f;background-color:#006000;">z</span><span style="color:#f8f8f2;background-color:#22a322;">)</span><span style="background-color:#22a322;"></span>
</pre>
<div class="code-caption">
<a href="https://github.com/SinglePages/NeuralNetworks/blob/main/Source/Code/Python/04-03-NeuronVectorized.py">Link
to code.</a>
</div>
</div>
<details class="question">
<summary>
<strong>Question:</strong> What are the dimensions of <span
class="math inline">\(\vz\)</span> and <span
class="math inline">\(\va\)</span> (aka, <span
class="math inline">\(\vyhat\)</span>)?
</summary>
<div class="answer">
<p><strong>Answer:</strong> We are computing a single output value for
each input, so, the shape of these vectors are <span
class="math inline">\((N \times 1)\)</span>. PyTorch will treat these as
arrays with <span class="math inline">\(N\)</span> elements instead of
as column vectors. <span class="math display">\[\begin{align}
\vz &amp;=
\begin{bmatrix}
    \vx^{(1)T} \vw + b \\
    \vx^{(2)T} \vw + b \\
    \vdots \\
    \vx^{(N)T} \vw + b \\
\end{bmatrix}
\\
\va &amp;=
\begin{bmatrix}
    g(z^{(1)}) \\
    g(z^{(2)}) \\
    \vdots \\
    g(z^{(N)}) \\
\end{bmatrix}

\end{align}\]</span></p>
</div>
</details>
<p>In the code snippet above, a matrix multiplication is indicated in
PyTorch using the <code>@</code> symbol (a <code>*</code> is used for
element-wise multiplications). A key to understanding matrix math is to
examine the shapes of all matrices involved. Above, <span
class="math inline">\(X\)</span> has a shape of <span
class="math inline">\((N \times n_x)\)</span>, <span
class="math inline">\(\vw\)</span> has a shape of <span
class="math inline">\((n_x \times 1)\)</span>, and <span
class="math inline">\(b\)</span> is a scalar multiplied by an
appropriately-shaped matrix of all ones (so that we can add <span
class="math inline">\(b\)</span> to each element of the <span
class="math inline">\(X\vw\)</span> result). Inner dimensions (the last
dimension of the left matrix and the first dimension of the right
matrix) must be the same for any valid matrix multiplication.</p>
<p>In the code snippet, the scalar <span
class="math inline">\(b\)</span> is added element-wise to every element
in the final matrix due to <a
href="https://pytorch.org/docs/stable/notes/broadcasting.html">broadcasting</a>
(this is a common library feature, not necessarily standard linear
algebra).</p>
<p>So far, we have random parameters and we ignore the output. But what
if we want to train the neuron so that the output mimics a real function
or process? The next subsection tackles this very problem.</p>
</section>
<section id="optimization-with-batch-gradient-descent" class="level2"
data-number="4.5">
<h2 data-number="4.5"><span class="header-section-number">4.5</span>
Optimization with Batch Gradient Descent</h2>
<p>We must find values for parameters <span
class="math inline">\(\vw\)</span> and <span
class="math inline">\(b\)</span> to make <span
class="math inline">\(\yhat\i \approx y\i\)</span>. As you might expect
from the title of this subsection, we are going to use gradient descent
to optimize the parameters. This means that we are going to need an
objective function (something to minimize) and to compute some
derivatives.</p>
<p>But what is an appropriate objective function (I’ll refer to this as
the <em>loss</em> function going forward)? How about the
<strong>mean-difference</strong>?</p>
<p><span class="math display">\[ℒ(\vyhat, \vy) = \sum_{i=1}^N (\yhat\i -
y\i) \quad \color{red}{\text{Don&#39;t use this loss
function.}}\]</span></p>
<details class="question">
<summary>
<strong>Question:</strong> What is problematic about this loss function?
</summary>
<div class="answer">
<p><strong>Answer:</strong></p>
<p>Let’s start by looking at the output of the function for different
values of the inputs.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><span
class="math inline">\(\yhat\i\)</span></th>
<th style="text-align: center;"><span
class="math inline">\(y\i\)</span></th>
<th style="text-align: center;">ℒ</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="even">
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-0.9</td>
</tr>
<tr class="odd">
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr class="even">
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-0.1</td>
</tr>
</tbody>
</table>
The table indicates that loss can be positive or negative. But how
should we interpret negative loss? We see that <span
class="math inline">\(ℒ\)</span> is minimized in row 2 of the table, but
this is not an ideal result. The sign of loss is not helpful—as we’ll
see shortly, we will use the sign of the derivative.
</div>
</details>
<p>A quick “fix” for the above loss function is to change it into the
<strong>mean-absolute-error</strong> (MAE):</p>
<p><span class="math display">\[\begin{align}
ℒ(\vyhat, \vy) &amp;= \sum_{i=1}^N |\yhat\i - y\i|\\
  &amp;= \mae
\end{align}\]</span></p>
<p>The second line shows a vectorized version using the L1-norm, which
is the sum of the absolute values of the given vector. MAE is a good
choice if your dataset includes outliers. MAE is also simple to
interpret: it is the average deviation between your models guess and the
correct answer.</p>
<p>A common choice for a loss function when training a regression model
is <strong>Half mean-square-error</strong> (Half-MSE):</p>
<p><span class="math display">\[\begin{align}
ℒ(\vyhat, \vy) &amp;= \frac{1}{2N} \sum_{i=1}^N (\yhat\i - y\i)^2\\
  &amp;= \vhmse
\end{align}\]</span></p>
<p>We are again using the L1-norm, but this time the vector we are
norming is the element-wise squared values of the difference between the
vectors <span class="math inline">\(\vyhat\)</span> and <span
class="math inline">\(\vy\)</span>. Interpreting Half-MSE is a bit
harder than MAE—you should multiply the result by two and then take the
square-root.</p>
<details class="question">
<summary>
<strong>Question:</strong> Why might we compute the half-MSE instead of
MSE or sum-square-error (SSE)?
</summary>
<div class="answer">
<strong>Answer:</strong> The factor cancels out when we take the
derivative. This scaling factor is unimportant since we will later
multiply it by a learning rate, and can use that to achieve whatever
effect we want.
</div>
</details>
<p>The standard choice when performing classification with a neuron is
<strong>binary cross-entropy</strong> (BCE):</p>
<p><span class="math display">\[\begin{align}
ℒ(\vyhat, \vy) &amp;= \vbce\\
  &amp;= -\text{mean}_0\left(\vy \cdot \log{\vyhat} + (1 - \vy) \cdot
\log{(1 - \vyhat)}\right)
\end{align}\]</span></p>
<p>In the vectorized version of BCE, I’ve used the non-standard <span
class="math inline">\(\text{mean}_0\)</span> notation to indicate that
we’re taking the average across the rows, dimension zero. This is closer
to the code that you’ll actually write. Very rarely will you want to put
a summation loop in your code.</p>
<details class="question">
<summary>
<strong>Question:</strong> Take some time to examine this loss function.
What happens for various values of <span
class="math inline">\(\yhat\i\)</span>, <span
class="math inline">\(y\i\)</span>?
</summary>
<div class="answer">
<p><strong>Answer:</strong></p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><span
class="math inline">\(\yhat\i\)</span></th>
<th style="text-align: center;"><span
class="math inline">\(y\i\)</span></th>
<th style="text-align: center;"><span
class="math inline">\(\log{\yhat\i}\)</span></th>
<th style="text-align: center;"><span
class="math inline">\(\log{(1-\yhat\i)}\)</span></th>
<th style="text-align: center;">ℒ</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">-2.3</td>
<td style="text-align: center;">-0.1</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr class="even">
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-2.3</td>
<td style="text-align: center;">-0.1</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr class="odd">
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">-0.1</td>
<td style="text-align: center;">-2.3</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr class="even">
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-0.1</td>
<td style="text-align: center;">-2.3</td>
<td style="text-align: center;">0.1</td>
</tr>
</tbody>
</table>
<p>The tables shows that a larger difference between <span
class="math inline">\(\yhat\i\)</span> and <span
class="math inline">\(y\i\)</span> (rows 2 and 3) results in a larger
loss, which is exactly what we’d like to see.</p>
</div>
</details>
<p>Let’s move forward using binary cross-entropy loss and the sigmoid
activation function.</p>
<p>We can only reduce loss by adjusting parameters. It doesn’t make
sense, for example, to minimize loss by changing the input values <span
class="math inline">\(X\)</span> or the output targets <span
class="math inline">\(Y\)</span>. Take a look at the following
fictitious loss landscape.</p>
<figure>
<img src="img/LossLandscape.svg"
alt="The effect on loss ℒ of adjusting parameter w_k." />
<figcaption aria-hidden="true">The effect on loss <span
class="math inline">\(ℒ\)</span> of adjusting parameter <span
class="math inline">\(w_k\)</span>.</figcaption>
</figure>
<p>The diagram above shows a curve for loss as a function of a single
parameter, <span class="math inline">\(w_k\)</span>. For this figure,
we’ll momentarily ignore that we might have dozens (or thousands or
millions) of parameters. We want to find a new value for <span
class="math inline">\(w_k\)</span> such that loss is reduced. You might
wonder why I said “loss is reduced” instead of “loss is minimized.” You
might be familiar with techniques for finding an <strong>exact</strong>
answer using an analytical (aka closed-form) solution.</p>
<details class="question">
<summary>
<strong>Question:</strong> What should we do if we wanted to
<strong>minimize</strong> loss with respect to the parameter using an
analytical solution?
</summary>
<div class="answer">
<p><strong>Answer:</strong> We should take the derivative, set it equal
to zero, and then solve the set of linear equations. Here is an example
using linear regression, which is very similar to our single neuron.
Here is our model:</p>
<p><span class="math display">\[\vyhat = X \mathbf{θ},\]</span></p>
<p>where <span class="math inline">\(θ\)</span> is our vector of
parameters. Here is our loss function (half-SSE):</p>
<p><span class="math display">\[ℒ(\vyhat, \vy) = \frac{1}{2} ||(\vyhat -
\vy)^2||_1.\]</span></p>
<p>Now we can take the partial derivative of loss with respect to
parameters <span class="math inline">\(θ\)</span>. (Note that I
substitute for <span class="math inline">\(\vyhat\)</span> on the third
line.)</p>
<p><span class="math display">\[\begin{align}
\frac{∂ ℒ}{∂ \mathbf{θ}} &amp;=
  \frac{∂ ||\frac{1}{2} (\vyhat - \vy)^2||_1}{∂ \mathbf{θ}} \\
&amp;= ||\vyhat - \vy||_1 \frac{∂ \vyhat}{∂ \mathbf{θ}} \\
&amp;= ||X \mathbf{θ} - \vy||_1 \frac{∂ X \mathbf{θ}}{∂ \mathbf{θ}} \\
&amp;= ||X \mathbf{θ} - \vy||_1 X \\
&amp;= X^T X \mathbf{θ} - X^T \vy
\end{align}\]</span></p>
<p>We can now set this derivative to zero and solve for <span
class="math inline">\(\mathbf{θ}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\frac{∂ ℒ}{∂ \mathbf{θ}} &amp;= 0 \\
X^T X \mathbf{θ} - X^T \vy &amp;= 0 \\
X^T X \mathbf{θ} &amp;= X^T \vy
\end{align}\]</span></p>
<p>And now assuming that <span class="math inline">\(X^T X\)</span> is
invertible (that the columns are linearly independent).</p>
<p><span class="math display">\[\mathbf{θ}^* = (X^T X)^{-1} X^T
\vy\]</span></p>
<p>We now have an optimal solution (called <span
class="math inline">\(\mathbf{θ}^*\)</span>) that minimizes loss. (See
<a href="https://en.wikipedia.org/wiki/Ordinary_least_squares"
title="Ordinary least squares - Wikipedia">Ordinary least squares -
Wikipedia</a> for more details.)</p>
</div>
</details>
<p>For complex models, such as a neural network, analytical solutions
are sometimes too slow or complicated to compute. Instead, we use an
iterative (aka numerical) solution. You can think of numerical solutions
as finding a good enough approximate solution as opposed to the exact
correct solution. Surprisingly, the numerical solution is often more
general than the exact solution—we’ll discuss this in later
sections.</p>
<p>To determine <strong>how</strong> we should adjust parameters, we
start the same way as finding the exact location and take the partial
derivative of loss with respect to each parameter. Taking the single
neuron, binary cross-entropy loss, and the sigmoid activation function
the chain rule in matrix form is as follows.</p>
<p><span class="math display">\[\begin{align}
\frac{∂ ℒ}{∂ \vw} &amp;=
  \frac{∂ ℒ}{∂ \vyhat}
  \frac{∂ \vyhat}{∂ \vz}
  \frac{∂ \vz}{∂ \vw} \\
  &amp;= \frac{1}{N} X^T (\vyhat - \vy)\\[20pt]
\frac{∂ ℒ}{∂ b} &amp;=
  \frac{∂ ℒ}{∂ \vyhat}
  \frac{∂ \vyhat}{∂ \vz}
  \frac{∂ \vz}{∂ b} \\
  &amp;= \frac{1}{N} \sum_{i=1}^N (\yhat\i - y\i)
\end{align}\]</span></p>
<p>Don’t worry too much about the derivations or notation at this stage.
We’ll go into more details when we follow the same process for a full
neural network in the next section.</p>
<details class="question">
<summary>
<strong>Question:</strong> Why is it necessary to apply the chain rule?
And why did the chain rule appear as it does above?
</summary>
<div class="answer">
<p><strong>Answer:</strong> First, we cannot directly compute the
partial derivative of <span class="math inline">\(ℒ\)</span> with
respect to <span class="math inline">\(\vw\)</span> (or <span
class="math inline">\(b\)</span>). Second, we only apply the chain rule
to equations that have some form of dependency on the term in the first
denominator (<span class="math inline">\(\vw\)</span> and <span
class="math inline">\(b\)</span>). It is useful to look at the loss
function when we substitute in values for <span
class="math inline">\(\yhat\)</span> and then <span
class="math inline">\(z\)</span>.</p>
<p><span class="math display">\[ℒ(\vyhat, \vy) =
-\frac{1}{N}\sum_{i=1}^N (y\i \log{σ(\vx^{(i)T}\vw\i + b)} + (1 -
y\i)\log{(1-σ(\vx^{(i)T}\vw\i + b))})\]</span></p>
<p>In the above equation we can more easily see how the chain-rule comes
into play. The parameter <span class="math inline">\(\vw\)</span> is
nested within a call to <span class="math inline">\(σ\)</span> which is
nested within a call to <span class="math inline">\(\log\)</span> when
computing <span class="math inline">\(\frac{∂ ℒ}{∂ \vw}\)</span>.</p>
</div>
</details>
<details class="question">
<summary>
<strong>Question:</strong> What do we do with the partial derivatives
<span class="math inline">\(\frac{∂ ℒ}{∂ \vw}\)</span> and <span
class="math inline">\(\frac{∂ ℒ}{∂ b}\)</span>?
</summary>
<div class="answer">
<p><strong>Answer:</strong> We use these terms to update model
parameters.</p>
<p><span class="math display">\[\begin{align}
\vw &amp;:= \vw - η \frac{∂ ℒ}{∂ \vw} \\
b &amp;:= b - η \frac{∂ ℒ}{∂ b}
\end{align}\]</span></p>
</div>
</details>
<details class="question">
<summary>
<strong>Question:</strong> What is the derivative of the sigmoid
function, <span class="math inline">\(σ\)</span>?
</summary>
<div class="answer">
<p><strong>Answer:</strong></p>
<p><span class="math display">\[\begin{align}
\frac{d}{dz} σ(z) &amp;=
  \frac{d}{dz} \left(\frac{1}{1 + e^{-z}}\right) \\
&amp;= \frac{d}{dz} \left(1 + e^{-z} \right)^{-1} \\
&amp;= -(1 + e^{-z})^{-2}(-e^{-z}) \\
&amp;= \frac{e^{-z}}{\left(1 + e^{-z}\right)^2} \\
&amp;= \frac{1}{1 + e^{-z}\ } \cdot \frac{e^{-z}}{1 + e^{-z}}  \\
&amp;= \frac{1}{1 + e^{-z}\ } \cdot \frac{e^{-z} + 1 - 1}{1 +
e^{-z}}  \\
&amp;= \frac{1}{1 + e^{-z}\ } \cdot \left( \frac{1 + e^{-z}}{1 + e^{-z}}
- \frac{1}{1 + e^{-z}} \right) \\
&amp;= \frac{1}{1 + e^{-z}\ } \cdot \left( 1 - \frac{1}{1 + e^{-z}}
\right) \\
&amp;= σ(z) \cdot (1 - σ(z))
\end{align}\]</span></p>
</div>
</details>
<p>With the two update equations shown in the previous answer we have
everything we need to train our neuron model. Looking at these two
equations you might wonder about the purpose of <span
class="math inline">\(η\)</span> (i.e., the “learning rate”). This
factor enables us to tune how fast or slow we learn. If <span
class="math inline">\(η\)</span> is set too high we might not be able to
learn, and it it is set too low we might learn prohibitively slowly. We
will go into more details on optimization in sec. 7.</p>
</section>
<section id="neuron-batch-gradient-descent" class="level2"
data-number="4.6">
<h2 data-number="4.6"><span class="header-section-number">4.6</span>
Neuron Batch Gradient Descent</h2>
<p>Here is a complete example in which we train a neuron to classify
images as either being of the digit 1 or the digit 7. Data processing
details are hidden in the <code>get_binary_mnist_one_batch</code>
function, but you can find that <a
href="https://github.com/SinglePages/NeuralNetworks/blob/767c4a3e357ba757b2e39767b489d7c51d1688c7/Source/Code/Python/utilities.py#L69">code
in the repository for this guide</a>.</p>
<div class="code-highlight">
<pre><span style="color:#39424e;">   1</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;">!/usr/bin/env python</span>
<span style="color:#39424e;">   2</span> 
<span style="color:#39424e;">   3</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">utilities</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">get_binary_mnist_one_batch</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">format_duration_with_prefix</span>
<span style="color:#39424e;">   4</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">timeit</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">default_timer</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">as</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">timer</span>
<span style="color:#39424e;">   5</span> <span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span>
<span style="color:#39424e;">   6</span> 
<span style="color:#39424e;">   7</span> 
<span style="color:#39424e;">   8</span> <span style="color:#ffa759;">def</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">compute_accuracy</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">yhat</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">y</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">:</span>
<span style="color:#39424e;">   9</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">valid_N</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">y</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">shape</span><span style="color:#ccc9c2;">[</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">]</span>
<span style="color:#39424e;">  10</span> <span style="color:#ccc9c2;">    </span><span style="color:#ffa759;">return</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">round</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">yhat</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">y</span><span style="color:#ccc9c2;">)</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">abs</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">sum</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">/</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">valid_N</span>
<span style="color:#39424e;">  11</span> 
<span style="color:#39424e;">  12</span> 
<span style="color:#39424e;">  13</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Get training and validation data loaders for classes A and B</span>
<span style="color:#39424e;">  14</span> <span style="color:#ccc9c2;">data_dir</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">../../Data</span><span style="color:#bae67e;">&quot;</span>
<span style="color:#39424e;">  15</span> <span style="color:#ccc9c2;">classA</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">classB</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;">, </span><span style="color:#ffcc66;">7</span>
<span style="color:#39424e;">  16</span> <span style="color:#ccc9c2;">flatten</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">True</span>
<span style="color:#39424e;">  17</span> <span style="color:#ccc9c2;">train_X</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">train_y</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">valid_X</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">valid_y</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">get_binary_mnist_one_batch</span><span style="color:#ccc9c2;">(</span>
<span style="color:#39424e;">  18</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">data_dir</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">classA</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">classB</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">flatten</span>
<span style="color:#39424e;">  19</span> <span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  20</span> 
<span style="color:#39424e;">  21</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Neuron parameters</span>
<span style="color:#39424e;">  22</span> <span style="color:#ccc9c2;">nx</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">28</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">28</span>
<span style="color:#39424e;">  23</span> <span style="color:#ccc9c2;">w</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">randn</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">nx</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">.</span><span style="color:#ffcc66;">01</span>
<span style="color:#39424e;">  24</span> <span style="color:#ccc9c2;">b</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">zeros</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  25</span> 
<span style="color:#39424e;">  26</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Batch gradient descent hyper-parameters</span>
<span style="color:#39424e;">  27</span> <span style="color:#ccc9c2;">num_epochs</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">4</span>
<span style="color:#39424e;">  28</span> <span style="color:#ccc9c2;">learning_rate</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">.</span><span style="color:#ffcc66;">01</span>
<span style="color:#39424e;">  29</span> 
<span style="color:#39424e;">  30</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Compute initial accuracy (should be around 50%)</span>
<span style="color:#39424e;">  31</span> <span style="color:#ccc9c2;">valid_yhat</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">sigmoid</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_X</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">&#64;</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">w</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">b</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  32</span> <span style="color:#ccc9c2;">valid_accuracy</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">compute_accuracy</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_yhat</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">valid_y</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  33</span> <span style="color:#f28779;">print</span><span style="color:#ccc9c2;">(</span><span style="color:#ffa759;">f</span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">Accuracy before training: </span><span style="color:#ccc9c2;">{</span><span style="color:#ccc9c2;">valid_accuracy</span><span style="color:#ffcc66;">:.2f</span><span style="color:#ccc9c2;">}</span><span style="color:#bae67e;">&quot;</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  34</span> 
<span style="color:#39424e;">  35</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Learn values for w and b that minimize loss</span>
<span style="color:#39424e;">  36</span> <span style="color:#ffa759;">for</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">epoch</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">in</span><span style="color:#ccc9c2;"> </span><span style="color:#f28779;">range</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">num_epochs</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">:</span>
<span style="color:#39424e;">  37</span> 
<span style="color:#39424e;">  38</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">start</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">timer</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  39</span> 
<span style="color:#39424e;">  40</span> <span style="color:#ccc9c2;">    </span><span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Make predictions given current paramters and then compute loss</span>
<span style="color:#39424e;">  41</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">yhat</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">sigmoid</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">train_X</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">&#64;</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">w</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">b</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  42</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">losses</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">train_y</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">log</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">yhat</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">train_y</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">log</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">yhat</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  43</span> 
<span style="color:#39424e;">  44</span> <span style="color:#ccc9c2;">    </span><span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Compute derivatives for w and b (dz is common to both derivatives)</span>
<span style="color:#39424e;">  45</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">dz</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">yhat</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">train_y</span>
<span style="color:#39424e;">  46</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">dw</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">/</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">train_y</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">shape</span><span style="color:#ccc9c2;">[</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">]</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">dz</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">&#64;</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">train_X</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  47</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">db</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">dz</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">mean</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  48</span> 
<span style="color:#39424e;">  49</span> <span style="color:#ccc9c2;">    </span><span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Update parameters</span>
<span style="color:#39424e;">  50</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">w</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">learning_rate</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">dw</span>
<span style="color:#39424e;">  51</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">b</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">learning_rate</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">db</span>
<span style="color:#39424e;">  52</span> 
<span style="color:#39424e;">  53</span> <span style="color:#ccc9c2;">    </span><span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Report on progress</span>
<span style="color:#39424e;">  54</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">valid_yhat</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">sigmoid</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_X</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">&#64;</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">w</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">b</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  55</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">valid_accuracy</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">compute_accuracy</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_yhat</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">valid_y</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  56</span> 
<span style="color:#39424e;">  57</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">info</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">f</span><span style="color:#bae67e;">&quot;</span><span style="color:#ccc9c2;">{</span><span style="color:#ccc9c2;">epoch</span><span style="color:#f29e74;">+</span><span style="color:#ffcc66;">1</span><span style="color:#ffcc66;">:&gt;2</span><span style="color:#ccc9c2;">}</span><span style="color:#bae67e;">/</span><span style="color:#ccc9c2;">{</span><span style="color:#ccc9c2;">num_epochs</span><span style="color:#ccc9c2;">}</span><span style="color:#bae67e;">&quot;</span>
<span style="color:#39424e;">  58</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">info</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">f</span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">, Loss=</span><span style="color:#ccc9c2;">{</span><span style="color:#ccc9c2;">losses</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">mean</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span><span style="color:#ffcc66;">:0.1f</span><span style="color:#ccc9c2;">}</span><span style="color:#bae67e;">&quot;</span>
<span style="color:#39424e;">  59</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">info</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">f</span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">, Accuracy=</span><span style="color:#ccc9c2;">{</span><span style="color:#ccc9c2;">valid_accuracy</span><span style="color:#ffcc66;">:.2f</span><span style="color:#ccc9c2;">}</span><span style="color:#bae67e;">&quot;</span>
<span style="color:#39424e;">  60</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">info</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">f</span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">, Time=</span><span style="color:#ccc9c2;">{</span><span style="color:#ffd580;">format_duration_with_prefix</span><span style="color:#ccc9c2;">(</span><span style="color:#ffd580;">timer</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;">start</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">}</span><span style="color:#bae67e;">&quot;</span>
<span style="color:#39424e;">  61</span> <span style="color:#ccc9c2;">    </span><span style="color:#f28779;">print</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">info</span><span style="color:#ccc9c2;">)</span>
</pre>
<div class="code-caption">
<a href="https://github.com/SinglePages/NeuralNetworks/blob/main/Source/Code/Python/04-04-NeuronMNIST.py">Link
to code.</a>
</div>
</div>
<details class="question">
<summary>
<strong>Question:</strong> Which lines of code correspond to <span
class="math inline">\(\frac{∂ ℒ}{∂ \vw}\)</span> and <span
class="math inline">\(\frac{∂ ℒ}{∂ b}\)</span>?
</summary>
<div class="answer">
<strong>Answer:</strong> Lines 46 and 47.
</div>
</details>
<details class="question">
<summary>
<strong>Question:</strong> What is an epoch?
</summary>
<div class="answer">
<strong>Answer:</strong> It turns out that we might need to update our
weights more than once to get useful results. Each time we update
parameters based on all training examples we mark the end of an epoch.
In the code above we iterate through four epochs.
</div>
</details>
<details class="question">
<summary>
<strong>Question:</strong> What do you expect to see for the output?
</summary>
<div class="answer">
<p><strong>Answer:</strong></p>
<pre class="code-block">
Accuracy before training: 0.54
 1/4, Loss=0.7, Accuracy=0.97, Time=5.5 ms
 2/4, Loss=0.5, Accuracy=0.96, Time=4.8 ms
 3/4, Loss=0.4, Accuracy=0.96, Time=4.6 ms
 4/4, Loss=0.3, Accuracy=0.96, Time=4.4 ms
</pre>
</div>
</details>
</section>
</section>
<section id="neural-networks-and-backpropagation" class="level1"
data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Neural
Networks and Backpropagation</h1>
<blockquote>
<p>Once your computer is pretending to be a neural net, you get it to be
able to do a particular task by just showing it a whole lot of
examples.</p>
<p>– Geoffrey Hinton</p>
</blockquote>
<p>Below is our first neural network (aka multi-layer perceptron, MLP).
We’ll start by using this diagram to formulate terminology and
conventions.</p>
<figure>
<img src="img/2LayerNetwork.svg" alt="A two-layer neural network." />
<figcaption aria-hidden="true">A two-layer neural network.</figcaption>
</figure>
<p>Notation:</p>
<ul>
<li>Layer 0 is the input (we called this <span
class="math inline">\(X\)</span> for a single Neuron)</li>
<li>Square bracket superscripts denote the network layer</li>
<li>Round parenthesis superscripts denote the example index</li>
<li><span class="math inline">\(w\)</span> parameter subscripts denote
first the associated neuron in the current layer and second the
associated neuron (or input) from the previous layer</li>
<li><span class="math inline">\(b\)</span>, <span
class="math inline">\(z\)</span>, and <span
class="math inline">\(a\)</span> subscripts denote an associated
neuron</li>
</ul>
<p>Notice how we have all the same components as we did for the single
neuron. We’ve just added additional notation to distinguish among layers
and neurons in the same layer.</p>
<details class="question">
<summary>
<strong>Question:</strong> Given some hypothetical deep neural network,
how would you denote the linear computation of the third neuron in the
fifth layer for training example 6123?
</summary>
<div class="answer">
<p><strong>Answer:</strong> <span
class="math display">\[z_3^{[5](6123)}\]</span></p>
<ul>
<li>“<span class="math inline">\(z\)</span>”: linear computation</li>
<li>“<span class="math inline">\([5]\)</span>” superscript: fifth
layer</li>
<li>“<span class="math inline">\((6123)\)</span>” superscript: example
6123</li>
<li>“<span class="math inline">\(3\)</span>” subscript: third
neuron</li>
</ul>
</div>
</details>
<section id="vectorized-equations-for-a-neural-network" class="level2"
data-number="5.1">
<h2 data-number="5.1"><span class="header-section-number">5.1</span>
Vectorized Equations For a Neural Network</h2>
<p>Let’s start with showing the notation for parameters from any layer
<span class="math inline">\(l = 1, 2, ..., L\)</span> where <span
class="math inline">\(L\)</span> is the number of layers in the
network.</p>
<p><span class="math display">\[\begin{align}
W^{[l]} &amp;=
\begin{bmatrix}
    w_{1,1}^{[l]} &amp; w_{1,2}^{[l]} &amp; \cdots
&amp;  w_{1,n_{l-1}}^{[l]} \\
    w_{2,1}^{[l]} &amp; w_{2,2}^{[l]} &amp; \cdots
&amp;  w_{2,n_{l-1}}^{[l]} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
    w_{n_l,1}^{[l]} &amp; w_{n_l,2}^{[l]} &amp; \cdots
&amp;  w_{n_l,n_{l-1}}^{[l]}
\end{bmatrix}
\\
\vb^{[l]} &amp;=
\begin{bmatrix}
    b_{1}^{[l]} \\
    b_{2}^{[l]} \\
    \vdots \\
    b_{n_l}^{[l]} \\
\end{bmatrix}
\end{align}\]</span></p>
<p>Compare these equations to the diagram above. Notice how the top
neuron in layer 1 would have its associated parameters in the first row
of <span class="math inline">\(W^{[1]}\)</span> and the first value in
<span class="math inline">\(\vb^{[1]}\)</span>.</p>
<p>Next we have the vectorized linear and activation equations for each
neuron in a layer (these are for all training examples):</p>
<p><span class="math display">\[\begin{align}
Z^{[l]} &amp;= A^{[l-1]} W^{[l]T} + \mathbf{1} \vb^{[l]T}\\
A^{[l]} &amp;= g^{[l]}(Z^{[l]})
\end{align}\]</span></p>
<details class="question">
<summary>
<strong>Question:</strong> Why do we have <span
class="math inline">\(\mathbf{1} \vb^{[l]T}\)</span>?
</summary>
<div class="answer">
<p><strong>Answer:</strong> This ensures that the dimensions are correct
between the added matrices. Try this out in Python:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>N, nl <span class="op">=</span> <span class="dv">10</span>, <span class="dv">4</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(nl, <span class="dv">1</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>ONE <span class="op">=</span> torch.ones(N, <span class="dv">1</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ONE <span class="op">@</span> b.T)</span></code></pre></div>
<p>Note that most neural network frameworks handle this for you in the
form of <a
href="https://pytorch.org/docs/stable/notes/broadcasting.html">broadcasting</a>.</p>
</div>
</details>
<details class="question">
<summary>
<strong>Question:</strong> What is the shape of <span
class="math inline">\(Z^{[l]}\)</span>?
</summary>
<div class="answer">
<p><strong>Answer:</strong> <span class="math inline">\(Z^{[l]}\)</span>
is <span class="math inline">\((N \times n_l)\)</span>. <span
class="math display">\[Z^{[l]} =
\begin{bmatrix}
    z_{1}^{[l](1)} &amp; z_{2}^{[l](1)} &amp; \cdots
&amp;  z_{n_l}^{[l](1)} \\
    z_{1}^{[l](2)} &amp; z_{2}^{[l](2)} &amp; \cdots
&amp;  z_{n_l}^{[l](2)} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
    z_{1}^{[l](N)} &amp; z_{2}^{[l](N)} &amp; \cdots
&amp;  z_{n_l}^{[l](N)}
\end{bmatrix}
\]</span></p>
<p>We compute this matrix by multiplying a <span
class="math inline">\((N \times n_{l-1})\)</span> matrix by a <span
class="math inline">\((n_{l-1}, n_l)\)</span> matrix (the transposed
parameter matrix) and adding an <span class="math inline">\((N \times
n_l)\)</span> matrix.</p>
</div>
</details>
<details class="question">
<summary>
<strong>Question:</strong> What is the shape of <span
class="math inline">\(A^{[l]}\)</span>?
</summary>
<div class="answer">
<p><strong>Answer:</strong> <span class="math inline">\(A^{[l]}\)</span>
is <span class="math inline">\((N \times n_l)\)</span>. <span
class="math display">\[\begin{align}
A^{[l]} &amp;=
\begin{bmatrix}
    a_{1}^{[l](1)} &amp; a_{2}^{[l](1)} &amp; \cdots
&amp;  a_{n_l}^{[l](1)} \\
    a_{1}^{[l](2)} &amp; a_{2}^{[l](2)} &amp; \cdots
&amp;  a_{n_l}^{[l](2)} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
    a_{1}^{[l](N)} &amp; a_{2}^{[l](N)} &amp; \cdots
&amp;  a_{n_l}^{[l](N)}
\end{bmatrix}
\\
\\
&amp;=
\begin{bmatrix}
    g_{1}^{[l]}(z_{1}^{[l](1)}) &amp; g_{2}^{[l]}(z_{2}^{[l](1)}) &amp;
\cdots &amp;  g_{n_l}^{[l]}(z_{n_l}^{[l](1)}) \\
    g_{1}^{[l]}(z_{1}^{[l](2)}) &amp; g_{2}^{[l]}(z_{2}^{[l](2)}) &amp;
\cdots &amp;  g_{n_l}^{[l]}(z_{n_l}^{[l](2)}) \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
    g_{1}^{[l]}(z_{1}^{[l](N)}) &amp; g_{2}^{[l]}(z_{2}^{[l](N)}) &amp;
\cdots &amp;  g_{n_l}^{[l]}(z_{n_l}^{[l](N)})
\end{bmatrix}
\\
\\
&amp;=
\begin{bmatrix}
    g_{1}^{[l]}(\va^{[l-1](1)} \vw_{1}^{[l]T} + b_{1}^{[l]}) &amp;
g_{2}^{[l]}(\va^{[l-1](1)} \vw_{2}^{[l]T} + b_{2}^{[l]}) &amp; \cdots
&amp;  g_{n_l}^{[l]}(\va^{[l-1](1)} \vw_{n_l}^{[l]T} + b_{n_l}^{[l]}) \\
    g_{1}^{[l]}(\va^{[l-1](2)} \vw_{1}^{[l]T} + b_{1}^{[l]}) &amp;
g_{2}^{[l]}(\va^{[l-1](2)} \vw_{2}^{[l]T} + b_{2}^{[l]}) &amp; \cdots
&amp;  g_{n_l}^{[l]}(\va^{[l-1](2)} \vw_{n_l}^{[l]T} + b_{n_l}^{[l]}) \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
    g_{1}^{[l]}(\va^{[l-1](N)} \vw_{1}^{[l]T} + b_{1}^{[l]}) &amp;
g_{2}^{[l]}(\va^{[l-1](N)} \vw_{2}^{[l]T} + b_{2}^{[l]}) &amp; \cdots
&amp;  g_{n_l}^{[l]}(\va^{[l-1](N)} \vw_{n_l}^{[l]T} + b_{n_l}^{[l]})
\end{bmatrix}
\end{align}\]</span></p>
<p>You should also think about the shapes of <span
class="math inline">\(\va^{[l-1](i)}\)</span> and <span
class="math inline">\(\vw_{j}^{[l]}\)</span>.</p>
</div>
</details>
</section>
<section id="backpropagation" class="level2" data-number="5.2">
<h2 data-number="5.2"><span class="header-section-number">5.2</span>
Backpropagation</h2>
<p>Just like for the single neuron, we want to find values for <span
class="math inline">\(W^{[l]}\)</span> and <span
class="math inline">\(\vb^{[l]}\)</span> (for <span
class="math inline">\(l = 1, 2, ..., L\)</span>) such that <span
class="math inline">\(A^{[L]} \approx Y\)</span> (<span
class="math inline">\(A^{[L]}\)</span> is another name for <span
class="math inline">\(\hat Y\)</span>). Instead of looking at a more
general case, let’s work through gradient descent for the two-layer
network above where we</p>
<ul>
<li>have three inputs (<span
class="math inline">\(n_x=n_0=3\)</span>),</li>
<li>have two neurons in layer 1 (<span
class="math inline">\(n_1=2\)</span>),</li>
<li>have three neurons in layer 2 (<span
class="math inline">\(n_y=n_2=3\)</span>),</li>
<li>are using sigmoid activations for all neurons, and</li>
<li>are using the binary-cross-entropy (BCE) loss function.</li>
</ul>
<p>You can imagine that we are performing multi-label classification.
For this network, we need to compute these partial derivatives:</p>
<p><span class="math display">\[
\frac{∂ℒ}{∂ W^{[1]}}^①,
\frac{∂ℒ}{∂ \vb^{[1]}}^②,
\frac{∂ℒ}{∂ W^{[2]}}^③,
\frac{∂ℒ}{∂ \vb^{[2]}}^④
\]</span></p>
<p>We are going to start at layer 2 and work backward through the
network to layer 1. As we compute these derivatives answer for yourself
<strong>“why do we work backward through the network?”</strong></p>
<p>This process of computing derivatives backward through the network is
why this process if referred to as backpropagation–we’ll compute values
and propagate them backward to earlier layers in the network. This is
easier to see when viewing the compute graph. A compute graph depicts
the flow of activations (during the forward pass) and gradients (during
the backward pass) through the network.</p>
<figure>
<img src="img/ComputeGraph.svg"
alt="Compute graph for two-layer network." />
<figcaption aria-hidden="true">Compute graph for two-layer
network.</figcaption>
</figure>
<p>Notice how the input flows forward from top-to-bottom in the compute
graph, but gradients flow backward (from bottom-to-top). This image
corresponds to the network above if you rotate it 90 degrees
anti-clockwise (mostly just so we I had space for the image on this
page).</p>
<section id="layer-2-parameters" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1"><span class="header-section-number">5.2.1</span>
Layer 2 Parameters</h3>
<p>Let’s start with the terms labeled ③ and ④ above, which correspond to
layer 2. The chain-rule requires us to derive three components.</p>
<p><span class="math display">\[\begin{align}
\frac{∂ℒ}{∂ W^{[2]}}^③ &amp;=
    \textcolor{blue}{\frac{∂ ℒ}{∂ A^{[2]}}}
    \textcolor{green}{\frac{∂ A^{[2]}}{∂ Z^{[2]}}}
    \frac{∂ Z^{[2]}}{∂ W^{[2]}}\\
\frac{∂ ℒ}{∂ \vb^{[2]}}^④ &amp;=
    \textcolor{blue}{\frac{∂ ℒ}{∂ A^{[2]}}}
    \textcolor{green}{\frac{∂ A^{[2]}}{∂ Z^{[2]}}}
    \frac{∂ Z^{[2]}}{∂ \vb^{[2]}}
\end{align}\]</span></p>
<p>These equations share the first two terms. In fact, we’ll see these
again for the first layer; so, it makes sense to give them their own
symbol, <span
class="math inline">\(∂_{Z^{[2]}}=\textcolor{blue}{{\frac{∂ ℒ}{∂
A^{[2]}}}}\textcolor{green}{\frac{∂ A^{[2]}}{∂ Z^{[2]}}}\)</span>. (You
might notice that I am leaving out the <span
class="math inline">\(\text{mean}_0\)</span> operation from BCE; this is
intentional as it will be handled below using a matrix multiplication
for one of the partial derivatives below.)</p>
<p><span class="math display">\[\begin{align}
\textcolor{blue}{\frac{∂ ℒ}{∂ A^{[2]}}} &amp;=
    -\frac{∂}{∂ A^{[2]}} \left(Y \cdot \log{A^{[2]}} + (1 - Y) \cdot
\log{\left(1 - A^{[2]}\right)}\right)\\
    &amp;= \left( \frac{1-Y}{1-A^{[2]}} - \frac{Y}{A^{[2]}}
\right)\\[20pt]

\textcolor{green}{\frac{∂ A^{[2]}}{∂ Z^{[2]}}} &amp;=
    \frac{∂}{∂ Z^{[2]}} σ(Z^{[2]})\\
    &amp;= σ(Z^{[2]}) \cdot (1 - σ(Z^{[2]}))\\
    &amp;= A^{[2]} \cdot (1 - A^{[2]})
\end{align}\]</span></p>
<p>Now substituting to solve for <span
class="math inline">\(∂_{Z^{[2]}}\)</span>.</p>
<p><span class="math display">\[\begin{align}
∂_{Z^{[2]}} &amp;=
    \left(\frac{1-Y}{1-A^{[2]}} - \frac{Y}{A^{[2]}}\right) A^{[2]} \cdot
(1 - A^{[2]})\\
    &amp;= (1-Y) \cdot A^{[2]} - Y \cdot (1 - A^{[2]})\\
    &amp;= A^{[2]} - Y \cdot A^{[2]} - Y + Y \cdot A^{[2]}\\
    &amp;= A^{[2]} - Y
\end{align}\]</span></p>
<p>Next we can solve the third terms in equations ③ and ④.</p>
<p><span class="math display">\[\begin{align}
\frac{∂ Z^{[2]}}{∂ W^{[2]}} &amp;=
    \frac{∂}{∂ W^{[2]}} A^{[1]} W^{[2]T} + \mathbf{1} \vb^{[2]T}\\
    &amp;= A^{[1]}\\[20pt]
\frac{∂ Z^{[2]}}{∂ \vb^{[2]}} &amp;=
    \frac{∂}{∂ \vb^{[2]}} A^{[1]} W^{[2]T} + \mathbf{1} \vb^{[2]T}\\
    &amp;= \mathbf{1}
\end{align}\]</span></p>
<p>And that leaves us with the following partial derivatives for ③ and
④.</p>
<p><span class="math display">\[\begin{align}
\frac{∂ℒ}{∂ W^{[2]}}^③ &amp;= \frac{1}{N} ∂_{Z^{[2]}}^T A^{[1]}\\
\frac{∂ ℒ}{∂ \vb^{[2]}}^④ &amp;= \text{mean}_0 (∂_{\vz^{[2]}})
\end{align}\]</span></p>
</section>
<section id="layer-1-parameters" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2"><span class="header-section-number">5.2.2</span>
Layer 1 Parameters</h3>
<p>Now we can continue to layer 1 and derive equations for terms ① and
②.</p>
<p><span class="math display">\[\begin{align}
\frac{∂ ℒ}{∂ W^{[1]}}^① &amp;=
    \textcolor{blue}{\frac{∂ ℒ}{∂ A^{[2]}}}
    \textcolor{green}{\frac{∂ A^{[2]}}{∂ Z^{[2]}}}
    \textcolor{cyan}{\frac{∂ Z^{[2]}}{∂ A^{[1]}}}
    \textcolor{lime}{\frac{∂ A^{[1]}}{∂ Z^{[1]}}}
    \frac{∂ Z^{[1]}}{∂ W^{[1]}}\\
&amp;= ∂_{Z^{[2]}}
    \frac{∂ Z^{[2]}}{∂ A^{[1]}}
    \frac{∂ A^{[1]}}{∂ Z^{[1]}}
    \frac{∂ Z^{[1]}}{∂ W^{[1]}}\\[20pt]

\frac{∂ ℒ}{∂ \vb^{[1]}}^② &amp;=
    \textcolor{blue}{\frac{∂ ℒ}{∂ A^{[2]}}}
    \textcolor{green}{\frac{∂ A^{[2]}}{∂ Z^{[2]}}}
    \textcolor{cyan}{\frac{∂ Z^{[2]}}{∂ A^{[1]}}}
    \textcolor{lime}{\frac{∂ A^{[1]}}{∂ Z^{[1]}}}
    \frac{∂ Z^{[1]}}{∂ \vb^{[1]}}\\
&amp;= ∂_{Z^{[2]}}
    \textcolor{cyan}{\frac{∂ Z^{[2]}}{∂ A^{[1]}}}
    \textcolor{lime}{\frac{∂ A^{[1]}}{∂ Z^{[1]}}}
    \frac{∂ Z^{[1]}}{∂ \vb^{[1]}}
\end{align}\]</span></p>
<p>All four derivations share the first two terms in common, <span
class="math inline">\(∂_{Z^{[2]}}\)</span>. The first layer parameters
additional share the next two terms. We’ll group the first four terms
together just like we did for layer 2: <span
class="math inline">\(∂_{Z^{[1]}}=∂_{Z^{[2]}}\textcolor{cyan}{\frac{∂
Z^{[2]}}{∂ A^{[1]}}}\textcolor{lime}{\frac{∂ A^{[1]}}{∂
Z^{[1]}}}\)</span>.</p>
<p>Let’s start by deriving this shared term.</p>
<p><span class="math display">\[\begin{align}
\textcolor{cyan}{\frac{∂ Z^{[2]}}{∂ A^{[1]}}} &amp;=
    \frac{∂}{A^{[1]}} (A^{[1]} W^{[2]T} + \mathbf{1} \vb^{[2]T})\\
    &amp;= W^{[2]}\\[20pt]

\textcolor{lime}{\frac{∂ A^{[1]}}{∂ Z^{[1]}}} &amp;= \frac{∂}{Z^{[1]}}\\
    &amp;= σ(Z^{[1]})\\
    &amp;= σ(Z^{[1]}) \cdot (1 - σ(Z^{[1]})\\
    &amp;= A^{[1]} \cdot (1-A^{[1]})
\end{align}\]</span></p>
<p>Now substituting to solve for <span
class="math inline">\(∂_{Z^{[1]}}\)</span>.</p>
<p><span class="math display">\[∂_{Z^{[1]}} = ∂_{Z^{[2]}} W^{[2]} \cdot
A^{[1]} \cdot (1 - A^{[1]})\]</span></p>
<p>We still have one term remaining for each of ① and ②.</p>
<p><span class="math display">\[\begin{align}
\frac{∂ Z^{[1]}}{∂ W^{[1]}} &amp;=
    \frac{∂}{∂ W^{[1]}} A^{[0]} W^{[1]T} + \mathbf{1} \vb^{[1]T}\\
    &amp;= A^{[0]}\\[20pt]
\frac{∂ Z^{[1]}}{∂ \vb^{[1]}} &amp;=
    \frac{∂}{\vb^{[1]}} A^{[0]} W^{[1]T} + \mathbf{1} \vb^{[1]T}\\
    &amp;= \mathbf{1}\\[20pt]
\end{align}\]</span></p>
<p>And that leaves us with the following partial derivatives for ① and
②.</p>
<p><span class="math display">\[\begin{align}
\frac{∂ℒ}{∂ W^{[1]}}^① &amp;= \frac{1}{N} ∂_{Z^{[1]}}^T A^{[0]}\\
\frac{∂ ℒ}{∂ \vb^{[1]}}^② &amp;= \text{mean}_0 (∂_{\vz^{[1](i)}})
\end{align}\]</span></p>
</section>
<section id="parameter-update-equations" class="level3"
data-number="5.2.3">
<h3 data-number="5.2.3"><span class="header-section-number">5.2.3</span>
Parameter Update Equations</h3>
<p>We can now write our update equations for all network parameters.</p>
<p><span class="math display">\[\begin{align}
W^{[1]} &amp;:= W^{[1]} - η \frac{∂ℒ}{∂ W^{[1]}} \\
    &amp;:= W^{[1]} - η \frac{1}{N} ∂_{Z^{[1]}}^T A^{[0]} \\[20pt]
\vb^{[1]} &amp;:= \vb^{[1]} - η \frac{∂ℒ}{∂ \vb^{[1]}} \\
    &amp;:= \vb^{[1]} - η\;\text{mean}_0 (∂_{\vz^{[1](i)}}) \\[20pt]
W^{[2]} &amp;:= W^{[2]} - η \frac{∂ℒ}{∂ W^{[2]}} \\
    &amp;:= W^{[2]} - η \frac{1}{N} ∂_{Z^{[2]}}^T A^{[1]}\\[20pt]
\vb^{[2]} &amp;:= \vb^{[2]} - η \frac{∂ℒ}{∂ \vb^{[2]}} \\
    &amp;:= \vb^{[2]} - η\;\text{mean}_0 (∂_{\vz^{[2](i)}})
\end{align}\]</span></p>
<details class="question">
<summary>
<strong>Question:</strong> Do these update equations need to be altered
if we want to change the loss function, activation functions, or network
architecture?
</summary>
<div class="answer">
<strong>Answer:</strong> Yes. Each of these factors play a part in the
derivations above.
</div>
</details>
</section>
</section>
<section id="neuron-batch-gradient-descent-1" class="level2"
data-number="5.3">
<h2 data-number="5.3"><span class="header-section-number">5.3</span>
Neuron Batch Gradient Descent</h2>
<p>Let’s put this together into an example similar to that shown in the
single neuron.</p>
<div class="code-highlight">
<pre><span style="color:#39424e;">   1</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;">!/usr/bin/env python</span>
<span style="color:#39424e;">   2</span> 
<span style="color:#39424e;">   3</span> <span style="color:#5c6773;">&quot;&quot;&quot;</span>
<span style="color:#39424e;">   4</span> <span style="color:#5c6773;">TODO(AJC): </span>
<span style="color:#39424e;">   5</span> <span style="color:#5c6773;">- Change X,y</span>
<span style="color:#39424e;">   6</span> <span style="color:#5c6773;">&quot;&quot;&quot;</span>
<span style="color:#39424e;">   7</span> 
<span style="color:#39424e;">   8</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">timeit</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">default_timer</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">as</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">timer</span>
<span style="color:#39424e;">   9</span> 
<span style="color:#39424e;">  10</span> <span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span>
<span style="color:#39424e;">  11</span> 
<span style="color:#39424e;">  12</span> <span style="color:#ffa759;">from</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">utilities</span><span style="color:#ccc9c2;"> </span><span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">format_duration_with_prefix</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">get_binary_mnist_one_batch</span>
<span style="color:#39424e;">  13</span> 
<span style="color:#39424e;">  14</span> 
<span style="color:#39424e;">  15</span> <span style="color:#ffa759;">def</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">compute_accuracy</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">prediction</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">target</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">:</span>
<span style="color:#39424e;">  16</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">valid_N</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">target</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">shape</span><span style="color:#ccc9c2;">[</span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">]</span>
<span style="color:#39424e;">  17</span> <span style="color:#ccc9c2;">    </span><span style="color:#ffa759;">return</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">round</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">prediction</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">target</span><span style="color:#ccc9c2;">)</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">abs</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">sum</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">/</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">valid_N</span>
<span style="color:#39424e;">  18</span> 
<span style="color:#39424e;">  19</span> 
<span style="color:#39424e;">  20</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Get training and validation data loaders for classes A and B</span>
<span style="color:#39424e;">  21</span> <span style="color:#ccc9c2;">data_dir</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">../../Data</span><span style="color:#bae67e;">&quot;</span>
<span style="color:#39424e;">  22</span> <span style="color:#ccc9c2;">classA</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">classB</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span><span style="color:#ccc9c2;">, </span><span style="color:#ffcc66;">7</span>
<span style="color:#39424e;">  23</span> <span style="color:#ccc9c2;">flatten</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">True</span>
<span style="color:#39424e;">  24</span> <span style="color:#ccc9c2;">train_X</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">train_y</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">valid_X</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">valid_y</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">get_binary_mnist_one_batch</span><span style="color:#ccc9c2;">(</span>
<span style="color:#39424e;">  25</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">data_dir</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">classA</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">classB</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">flatten</span>
<span style="color:#39424e;">  26</span> <span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  27</span> 
<span style="color:#39424e;">  28</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Neural network layer sizes for MNIST</span>
<span style="color:#39424e;">  29</span> <span style="color:#ccc9c2;">n0</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">28</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">28</span>
<span style="color:#39424e;">  30</span> <span style="color:#ccc9c2;">n1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">2</span>
<span style="color:#39424e;">  31</span> <span style="color:#ccc9c2;">n2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">1</span>
<span style="color:#39424e;">  32</span> 
<span style="color:#39424e;">  33</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Network parameters</span>
<span style="color:#39424e;">  34</span> <span style="color:#ccc9c2;">W1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">randn</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">n1</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">n0</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  35</span> <span style="color:#ccc9c2;">b1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">randn</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">n1</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  36</span> <span style="color:#ccc9c2;">W2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">randn</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">n2</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">n1</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  37</span> <span style="color:#ccc9c2;">b2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">randn</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">n2</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  38</span> 
<span style="color:#39424e;">  39</span> 
<span style="color:#39424e;">  40</span> <span style="color:#ffa759;">def</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">model</span><span style="color:#ccc9c2;">(</span><span style="color:#ffcc66;">A0</span><span style="color:#ccc9c2;">)</span><span style="color:#ccc9c2;">:</span>
<span style="color:#39424e;">  41</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">Z1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">A0</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">&#64;</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">W1</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">T</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">b1</span>
<span style="color:#39424e;">  42</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">A1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">sigmoid</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">Z1</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  43</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">Z2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">A1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">&#64;</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">W2</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">T</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">b2</span>
<span style="color:#39424e;">  44</span> <span style="color:#ccc9c2;">    </span><span style="color:#ccc9c2;">A2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">sigmoid</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">Z2</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  45</span> <span style="color:#ccc9c2;">    </span><span style="color:#ffa759;">return</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">Z1</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">A1</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">Z2</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">A2</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">squeeze</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  46</span> 
<span style="color:#39424e;">  47</span> 
<span style="color:#39424e;">  48</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Batch gradient descent hyper-parameters</span>
<span style="color:#39424e;">  49</span> <span style="color:#ccc9c2;">num_epochs</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">4</span>
<span style="color:#39424e;">  50</span> <span style="color:#ccc9c2;">learning_rate</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">0</span><span style="color:#ccc9c2;">.</span><span style="color:#ffcc66;">01</span>
<span style="color:#39424e;">  51</span> 
<span style="color:#39424e;">  52</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Compute initial accuracy (should be around 50%)</span>
<span style="color:#39424e;">  53</span> <span style="color:#5ccfe6;">_</span><span style="color:#ccc9c2;">, </span><span style="color:#5ccfe6;">_</span><span style="color:#ccc9c2;">, </span><span style="color:#5ccfe6;">_</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">valid_preds</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">model</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_X</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  54</span> <span style="color:#ccc9c2;">valid_accuracy</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffd580;">compute_accuracy</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_preds</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">valid_y</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  55</span> <span style="color:#f28779;">print</span><span style="color:#ccc9c2;">(</span><span style="color:#ffa759;">f</span><span style="color:#bae67e;">&quot;</span><span style="color:#bae67e;">Accuracy before training: </span><span style="color:#ccc9c2;">{</span><span style="color:#ccc9c2;">valid_accuracy</span><span style="color:#ffcc66;">:.2f</span><span style="color:#ccc9c2;">}</span><span style="color:#bae67e;">&quot;</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  56</span> <span style="color:#f28779;">print</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_preds</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">shape</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  57</span> <span style="color:#f28779;">print</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">valid_y</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">shape</span><span style="color:#ccc9c2;">)</span>
</pre>
<div class="code-caption">
<a href="https://github.com/SinglePages/NeuralNetworks/blob/main/Source/Code/Python/05-01-TwoLayerNeuralNetworkMNIST.py">Link
to code.</a>
</div>
</div>
</section>
<section id="automatic-differentiation" class="level2"
data-number="5.4">
<h2 data-number="5.4"><span class="header-section-number">5.4</span>
Automatic Differentiation</h2>
<p>Let’s agree we should avoid computing those derivatives by hand. The
process is time consuming and error prone. Instead let’s rely on a
technique known as <em>automatic differentiation</em>, which is built-in
to PyTorch and most machine learning frameworks.</p>
<p>An automatic differentiation library:</p>
<ol type="1">
<li>Creates a compute graph from your tensor operations.</li>
<li>Performs a topological sort on the compute graph.</li>
<li>Compute gradients and back propagates them to all matrices.</li>
</ol>
<p>Let’s take a look at an example.</p>
<div class="code-highlight">
<pre><span style="color:#39424e;">   1</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;">!/usr/bin/env python3</span>
<span style="color:#39424e;">   2</span> 
<span style="color:#39424e;">   3</span> <span style="color:#ffa759;">import</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span>
<span style="color:#39424e;">   4</span> 
<span style="color:#39424e;">   5</span> <span style="color:#ccc9c2;">N</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">n0</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">n1</span><span style="color:#ccc9c2;">, </span><span style="color:#ccc9c2;">n2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">20</span><span style="color:#ccc9c2;">, </span><span style="color:#ffcc66;">10</span><span style="color:#ccc9c2;">, </span><span style="color:#ffcc66;">7</span><span style="color:#ccc9c2;">, </span><span style="color:#ffcc66;">13</span>
<span style="color:#39424e;">   6</span> 
<span style="color:#39424e;">   7</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Some fake input</span>
<span style="color:#39424e;">   8</span> <span style="color:#ccc9c2;">A0</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">randn</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">N</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">n0</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">   9</span> <span style="color:#ccc9c2;">Y</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">randn</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">N</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">n2</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  10</span> 
<span style="color:#39424e;">  11</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Layer 1</span>
<span style="color:#39424e;">  12</span> <span style="color:#ccc9c2;">W1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">randn</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">n1</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">n0</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">requires_grad</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">True</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  13</span> <span style="color:#ccc9c2;">b1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">randn</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">n1</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">requires_grad</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">True</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  14</span> <span style="color:#ccc9c2;">temp1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">A0</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">&#64;</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">W1</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">T</span>
<span style="color:#39424e;">  15</span> <span style="color:#ccc9c2;">Z1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">temp1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">b1</span>
<span style="color:#39424e;">  16</span> <span style="color:#ccc9c2;">A1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">sigmoid</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">Z1</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  17</span> 
<span style="color:#39424e;">  18</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Layer 2</span>
<span style="color:#39424e;">  19</span> <span style="color:#ccc9c2;">W2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">randn</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">n2</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">n1</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">requires_grad</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">True</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  20</span> <span style="color:#ccc9c2;">b2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">randn</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">n2</span><span style="color:#ccc9c2;">,</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">requires_grad</span><span style="color:#f29e74;">=</span><span style="color:#ffcc66;">True</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  21</span> <span style="color:#ccc9c2;">temp2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">A1</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">&#64;</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">W2</span><span style="color:#f29e74;">.</span><span style="color:#ccc9c2;">T</span>
<span style="color:#39424e;">  22</span> <span style="color:#ccc9c2;">Z2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">temp2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">+</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">b2</span>
<span style="color:#39424e;">  23</span> <span style="color:#ccc9c2;">A2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">torch</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">sigmoid</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">Z2</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  24</span> 
<span style="color:#39424e;">  25</span> <span style="color:#5c6773;">#</span><span style="color:#5c6773;"> Loss and backward propagation</span>
<span style="color:#39424e;">  26</span> <span style="color:#ccc9c2;">temp3</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">A2</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">-</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">Y</span>
<span style="color:#39424e;">  27</span> <span style="color:#ccc9c2;">temp4</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">temp3</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">*</span><span style="color:#f29e74;">*</span><span style="color:#ccc9c2;"> </span><span style="color:#ffcc66;">2</span>
<span style="color:#39424e;">  28</span> <span style="color:#ccc9c2;">loss</span><span style="color:#ccc9c2;"> </span><span style="color:#f29e74;">=</span><span style="color:#ccc9c2;"> </span><span style="color:#ccc9c2;">temp4</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">mean</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span>
<span style="color:#39424e;">  29</span> <span style="color:#ccc9c2;">loss</span><span style="color:#f29e74;">.</span><span style="color:#ffd580;">backward</span><span style="color:#ccc9c2;">(</span><span style="color:#ccc9c2;">)</span>
</pre>
<div class="code-caption">
<a href="https://github.com/SinglePages/NeuralNetworks/blob/main/Source/Code/Python/05-02-AutomaticDifferentiation.py">Link
to code.</a>
</div>
</div>
<p>Two key points from the listing above, (1)
<code>requires_grad=True</code> tells PyTorch to create the compute
graph and compute partial derivatives with respect to the given tensor,
and (2) I’ve ensured that each line of code contains a single operation,
which makes it easier to match with the diagram below (I’ve provided
this one in a bit more detail).</p>
<figure>
<img src="img/AutoDiffComputeGraph.svg"
alt="Compute graph for two-layer network." />
<figcaption aria-hidden="true">Compute graph for two-layer
network.</figcaption>
</figure>
<p>This diagram is (often) constructed dynamically as operations are
performed. Edges indicate the flow of gradients in the backward
direction. We start at graph source nodes (e.g., the “loss” node) and
compute partial derivatives with respect to their inputs until we reach
graph sinks (e.g., parameters). This diagram (and the corresponding
code) map directly to the hand-computed derivatives from the previous
section. Take some time and see if you can see how they map to one
another.</p>
<p>If you’d like to see <em>how</em> an automatic differentiation
library is coded, please take a look at my simple <a
href="https://github.com/anthonyjclark/match">Match</a> library, which
tries to closely mimic the PyTorch interface.</p>
<section id="alternatives" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1"><span class="header-section-number">5.4.1</span>
Alternatives</h3>
<p>In addition to the technique above known as reverse mode automatic
differentiation, you might also hear about</p>
<ul>
<li><a href="https://mostafa-samir.github.io/auto-diff-pt1/">forward
mode automatic differentiation with dual numbers</a>,</li>
<li><a
href="https://en.wikipedia.org/wiki/Numerical_differentiation">numerical
differentiation (Wikipedia)</a>, and</li>
<li><a href="https://en.wikipedia.org/wiki/Computer_algebra">symbolic
differentiation (Wikipedia)</a>.</li>
</ul>
<p>These techniques are similar and have various deficiences and
advantages. Most modern libraries implement reverse mode automatic
differentiation.</p>
</section>
</section>
<section id="why-deep-neural-networks" class="level2" data-number="5.5">
<h2 data-number="5.5"><span class="header-section-number">5.5</span> Why
“Deep” Neural Networks?</h2>
</section>
<section id="the-role-of-an-activation-function" class="level2"
data-number="5.6">
<h2 data-number="5.6"><span class="header-section-number">5.6</span> The
Role of an Activation Function</h2>
</section>
</section>
<section id="gradient-descent" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span>
Gradient Descent</h1>
<p>Optimizing a neural network follows this process:</p>
<ol type="1">
<li>prepare dataset(s) (e.g., proxy, training, validation, evaluation,
etc.),</li>
<li>set hyperparameters (e.g., learning rate, number of epochs,
etc.),</li>
<li>create the model, and</li>
<li>train the model.</li>
</ol>
<p>We’ve discussed each of these areas in general, but now we’ll go into
more details starting with the final step, training the model.</p>
<section id="batch-gradient-descent" class="level2" data-number="6.1">
<h2 data-number="6.1"><span class="header-section-number">6.1</span>
Batch Gradient Descent</h2>
<p>All examples thus far have used batch gradient descent (BGD). All
gradient descent methods are iterative, meaning we continually make
small changes to the parameters until we are satisfied or run out of
time. BGD looks something like this:</p>
<pre class="text"><code>for each epoch
    1. compute gradient with respect to all examples
    2. average gradients across all examples
    3. update parameters using averaged gradients</code></pre>
<p>In all variants of gradient descent, an epoch refers to the process
by which we update the parameters with respect to all training examples.
In batch gradient descent, we compute all gradients at once and average
them across all examples, resulting in the parameters being updated a
single time each epoch. This has the advantage of smoothing out the
affect of any outliers and leveraging the parallel nature of modern CPUs
and GPUs. On the other hand, it can be a waste of resources (mainly
time) to only update the parameters once each epoch.</p>
</section>
<section id="stochastic-gradient-descent" class="level2"
data-number="6.2">
<h2 data-number="6.2"><span class="header-section-number">6.2</span>
Stochastic Gradient Descent</h2>
<p>In stochastic Gradient Descent (SGD) we update parameter <span
class="math inline">\(N\)</span> times per epoch—once per example. This
means that we update parameters more frequently than in BGD.</p>
<p>The <strong>stochastic</strong> part of SGD refers to a random
shuffling of the example each epoch. This tends to reduce loss “cycling”
where some sequence of repeated example increases and then decreases
loss.</p>
<pre class="text"><code>for each epoch
    randomly shuffle all examples
    for each example
        1. compute gradient with respect to single example
        2. update parameters using gradient</code></pre>
<p>Although we update the parameters more frequently, not all updates
are <em>good</em>. Outliers will make the model perform worse in the
general case. Moreover, SGD does not take advantage of parallel
computations.</p>
</section>
<section id="mini-batch-stochastic-gradient-descent" class="level2"
data-number="6.3">
<h2 data-number="6.3"><span class="header-section-number">6.3</span>
Mini-Batch Stochastic Gradient Descent</h2>
<p>Mini-Batch SGD provides a middle ground. We chunk the input into some
number of batches and take the average gradient over each batch.</p>
<pre class="text"><code>for each epoch
    randomly distribute examples into batches
    for each batch
        1. compute gradient with respect to all examples in batch
        2. average gradients across all examples in batch
        3. update parameters using averaged gradients</code></pre>
<p>This enables us to get the best of both worlds:</p>
<ul>
<li>less susceptible to outliers and noise,</li>
<li>a good number of updates per epoch, and</li>
<li>good utilization of computing resources.</li>
</ul>
<details class="question">
<summary>
<strong>Question:</strong> What batch size turns Mini-Batch SGD into
BGD? What batch size turns Mini-Batch SGD into SGD?
</summary>
<div class="answer">
<strong>Answer:</strong> <span class="math inline">\(N\)</span> and
<span class="math inline">\(1\)</span>, respectively.
</div>
</details>
<details class="question">
<summary>
<strong>Question:</strong> Will all batches be the same size?
</summary>
<div class="answer">
<strong>Answer:</strong> No. The last batch is frequently smaller than
all other batches. It contains the leftovers.
</div>
</details>
<p>The code-diff below shows how few changes are needed to convert our
BGD example into Mini-Batch SGD.</p>
</section>
</section>
<section id="sec:opti" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span>
Optimization Techniques</h1>
<section id="momentum" class="level2" data-number="7.1">
<h2 data-number="7.1"><span class="header-section-number">7.1</span>
Momentum</h2>
<section id="nesterovs-accelerated-gradients" class="level3"
data-number="7.1.1">
<h3 data-number="7.1.1"><span class="header-section-number">7.1.1</span>
Nesterov’s Accelerated Gradients</h3>
</section>
</section>
<section id="rmsprop" class="level2" data-number="7.2">
<h2 data-number="7.2"><span class="header-section-number">7.2</span>
RMSProp</h2>
</section>
<section id="adam" class="level2" data-number="7.3">
<h2 data-number="7.3"><span class="header-section-number">7.3</span>
Adam</h2>
</section>
<section id="amsgrad" class="level2" data-number="7.4">
<h2 data-number="7.4"><span class="header-section-number">7.4</span>
AMSGrad</h2>
</section>
</section>
<section id="sec:generalization" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span>
Overfitting and Generalization</h1>
<p><em>Being revised</em></p>
</section>
<section id="sec:cnns" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span>
Convolutional Neural Networks</h1>
<p><em>Being revised</em></p>
</section>
<section id="recurrent-neural-networks" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span>
Recurrent Neural Networks</h1>
<p><em>Being revised</em></p>
</section>
<section id="attention-and-transformers" class="level1"
data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span>
Attention and Transformers</h1>
<p><em>Being revised</em></p>
</section>
<section id="sec:hyper" class="level1" data-number="12">
<h1 data-number="12"><span class="header-section-number">12</span>
Advanced Topics</h1>
<p><em>Being revised</em></p>
</section>
<section id="sec:terms" class="level1 unnumbered">
<h1 class="unnumbered">Terminology</h1>
</section>
</body>
</html>
